{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C_02_NLP_Supervised_Machine_Learning_Model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPI+ajSEe5U/R2n12FgkoTv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_qQ-mhBa6-ZQ"},"outputs":[],"source":["## Sentiment Analysis Using Supervised Learning Approach\n","Purpose: To use Machine Learning Method, using TFIDF Vectorizer to vectorize the corpus, to predict labels\n","\n","# from keras.models import Sequential\n","# from keras.layers import Input, Dense, Embedding, SpatialDropout1D, LSTM\n","# from keras.callbacks import EarlyStopping\n","\n","from sklearn.model_selection import train_test_split, StratifiedKFold\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn import metrics\n","from sklearn.dummy import DummyClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import svm, tree\n","# from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","from xgboost import XGBClassifier\n","\n","from sklearn import model_selection\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.model_selection import RandomizedSearchCV\n","\n","from sklearn import preprocessing\n","from sklearn.pipeline import Pipeline\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","from sklearn.datasets import make_classification\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from matplotlib import pyplot\n","\n","import datetime\n","import os\n","import pandas as pd\n","import matplotlib.pyplot as plt \n","import numpy as np\n","from keras.preprocessing.text import Tokenizer\n","import string\n","import seaborn as sns\n","import spacy\n","import unidecode\n","# import contractions as contract\n","import re\n","import wordninja\n","import collections\n","import pkg_resources\n","from spellchecker import SpellChecker \n","from symspellpy import SymSpell, Verbosity\n","from wordcloud import WordCloud\n","\n","import pickle\n","\n","# Pickle Methodology\n","def load_pickle(input_dir):\n","    f = open(input_dir, \"rb\")\n","    df = pickle.load(f)\n","    return df\n","    \n","def save_pickle(df, output_dir):\n","    df.to_pickle(output_dir)\n","\n","# import labelled dataset from pre-labelled data in RavenPack\n","import datetime\n","\n","batch_id = datetime.date.today().strftime(\"%y%m%d\")\n","df = pd.read_csv(f'outputs/china_news_sentiments.csv', encoding='latin-1')\n","df = df[['timestamp_tz', 'cleaned_headline', 'class', 'entity_name']]\n","df = df[df['cleaned_headline'].notna()]\n","df.reset_index(inplace=True, drop=True)\n","df\n","\n","### Vectorize Textual Data\n","We convert textual data into numerical feature vectors\n","\n","# -1 (NEGATIVE)\n","# 1 (POSITIVE)\n","\n","le = LabelEncoder()\n","df['class_label'] = le.fit_transform(df['class'])\n","\n","X_train, X_test, y_train, y_test = train_test_split(df[['cleaned_headline']], df['class_label'], shuffle=True, test_size=0.30)\n","\n","### Classification Models\n","\n","### TfidfVectorizer For Corpus\n","\n","pipeline = Pipeline([\n","                     ('tfidf', TfidfVectorizer()),\n","                     ('logreg', LogisticRegression(solver='lbfgs'))\n","])\n","\n","pipeline.fit(X_train.cleaned_headline, y_train)\n","\n","metrics.accuracy_score(y_test, pipeline.predict(X_test.cleaned_headline))\n","\n","%%time\n","\n","parameters = {\n","    'tfidf__ngram_range': [(1, 1), (1, 2), (1,3)],\n","    'tfidf__max_df': np.linspace(0.16, 0.21, 6),\n","    'tfidf__min_df': range(7, 15),\n","}\n","\n","clf = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=5)\n","clf.fit(X_train.cleaned_headline, y_train)\n","\n","clf.best_params_\n","\n","#### Tune with an updated set of parameter space\n","(To provide a more conclusive results)\n","\n","parameters_updated = {\n","    'tfidf__ngram_range': [(1, 3), (1, 4), (1, 5)],\n","    'tfidf__max_df': np.linspace(0.10, 0.24, 6),\n","    'tfidf__min_df': range(1, 10),\n","}\n","\n","clf_updated = GridSearchCV(pipeline, parameters_updated, n_jobs=-1, verbose=5)\n","clf_updated.fit(X_train.cleaned_headline, y_train)\n","\n","clf_updated.best_params_\n","\n","#### Use the parameters to vectorize the Corpus column\n","\n","tfidf_vect = TfidfVectorizer(max_df=0.1, min_df=3, ngram_range=(1, 3)) # TfidfVectorizer(max_df=0.16, min_df=5, ngram_range=(1, 3))\n","X_train_dtm = tfidf_vect.fit_transform(X_train.cleaned_headline)\n","X_test_dtm = tfidf_vect.transform(X_test.cleaned_headline)\n","\n","### XGBoost Model\n","\n","%%time\n","\n","xg_params = {'max_depth': range(3,10,1), \n","             'min_child_weight': range(1,8,2)\n","            }\n","\n","xg = XGBClassifier()\n","\n","xg_clf = GridSearchCV(xg, xg_params, verbose=5, n_jobs=-1)\n","xg_clf.fit(X_train_dtm, y_train)\n","\n","metrics.accuracy_score(y_test, xg_clf.predict(X_test_dtm))\n","\n","xg_clf.best_params_\n","\n","%%time\n","\n","xg_params_updated = {'max_depth': range(1,11,2), \n","             'min_child_weight': range(1,21,2)\n","            }\n","\n","xg = XGBClassifier()\n","\n","xg_clf_updated = GridSearchCV(xg, xg_params_updated, verbose=5, n_jobs=-1)\n","xg_clf_updated.fit(X_train_dtm, y_train)\n","\n","xg_clf_updated.best_params_\n","\n","xg = XGBClassifier(max_depth=7, min_child_weight=1)\n","\n","xg.fit(X_train_dtm, y_train)\n","\n","metrics.accuracy_score(y_test, xg.predict(X_test_dtm))\n","\n","target_names = ['Negative', 'Positive']\n","\n","# Print the confusion matrix\n","print(metrics.confusion_matrix(y_test, xg.predict(X_test_dtm)))\n","\n","# Print the precision and recall, among other metrics\n","print(metrics.classification_report(y_test, xg.predict(X_test_dtm), digits=3, target_names=target_names))\n","\n","## for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","classes = np.unique(y_test)\n","fig, ax = plt.subplots()\n","cm = metrics.confusion_matrix(y_test, xg.predict(X_test_dtm), labels=classes)\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n","ax.set_yticklabels(labels=classes, rotation=0)\n","plt.show()\n","\n","# roc curve and auc\n","\n","def plot_roc_curve(fpr, tpr):\n","    plt.plot(fpr, tpr, color='orange', label='ROC')\n","    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    plt.show()\n","    \n","probs = xg.predict_proba(X_test_dtm)\n","probs = probs[:, 1]\n","auc = roc_auc_score(y_test, probs)\n","print('AUC: %.2f' % auc)\n","fpr, tpr, thresholds = roc_curve(y_test, probs)\n","plot_roc_curve(fpr, tpr)\n","\n","### SVM\n","Test both Radial and Linear Basis for SVM model training\n","Reference: https://www.researchgate.net/publication/305623869_Sentiment_Classification_of_Financial_News_Using_Statistical_Features\n","\n","%%time\n","\n","svm_params = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01]}\n","\n","sv = svm.SVC()\n","\n","svm_clf = GridSearchCV(sv, svm_params, verbose=5, n_jobs=-1)\n","svm_clf.fit(X_train_dtm, y_train)\n","\n","svm_clf.best_params_\n","\n","metrics.accuracy_score(y_test, svm_clf.predict(X_test_dtm))\n","\n","svm_params = {'C': [0.1, 1, 10, 100, 1000, 10000], 'gamma': [1000, 100, 10, 1, 0.1, 0.01]}\n","\n","sv = svm.SVC()\n","\n","svm_clf = GridSearchCV(sv, svm_params, verbose=5, n_jobs=-1)\n","svm_clf.fit(X_train_dtm, y_train)\n","svm_clf.best_params_\n","\n","sv = svm.SVC(C=10, gamma=0.1)\n","\n","sv.fit(X_train_dtm, y_train)\n","\n","metrics.accuracy_score(y_test, sv.predict(X_test_dtm))\n","\n","# Print the confusion matrix\n","print(metrics.confusion_matrix(y_test, sv.predict(X_test_dtm)))\n","\n","# Print the precision and recall, among other metrics\n","print(metrics.classification_report(y_test, sv.predict(X_test_dtm), digits=3, target_names=target_names))\n","\n","## for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","classes = np.unique(y_test)\n","fig, ax = plt.subplots()\n","cm = metrics.confusion_matrix(y_test, sv.predict(X_test_dtm), labels=classes)\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n","ax.set_yticklabels(labels=classes, rotation=0)\n","plt.show()\n","\n","# roc curve and auc\n","sv = svm.SVC(C=100, gamma=0.01, probability=True) # made probability=True\n","sv.fit(X_train_dtm, y_train)\n","\n","def plot_roc_curve(fpr, tpr):\n","    plt.plot(fpr, tpr, color='orange', label='ROC')\n","    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    plt.show()\n","    \n","probs = sv.predict_proba(X_test_dtm)\n","probs = probs[:, 1]\n","auc = roc_auc_score(y_test, probs)\n","print('AUC: %.2f' % auc)\n","fpr, tpr, thresholds = roc_curve(y_test, probs)\n","plot_roc_curve(fpr, tpr)\n","\n","### Logistic Regression\n","\n","lr = LogisticRegression()\n","\n","lr.fit(X_train_dtm, y_train)\n","\n","metrics.accuracy_score(y_test, lr.predict(X_test_dtm))\n","\n","parameters_logreg = {\n","    'penalty': ['l1', 'l2'],\n","    'C': [100, 10, 1.0, 0.1, 0.01]\n","}\n","\n","lr = LogisticRegression()\n","lr_clf = GridSearchCV(lr, parameters_logreg, verbose=5, n_jobs=-1)\n","lr_clf.fit(X_train_dtm, y_train)\n","lr_clf.best_params_\n","\n","lr = LogisticRegression(C=10, penalty='l2')\n","\n","lr.fit(X_train_dtm, y_train)\n","\n","metrics.accuracy_score(y_test, lr.predict(X_test_dtm))\n","\n","## for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","classes = np.unique(y_test)\n","fig, ax = plt.subplots()\n","cm = metrics.confusion_matrix(y_test, lr.predict(X_test_dtm), labels=classes)\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n","ax.set_yticklabels(labels=classes, rotation=0)\n","plt.show()\n","\n","# roc curve and auc\n","\n","def plot_roc_curve(fpr, tpr):\n","    plt.plot(fpr, tpr, color='orange', label='ROC')\n","    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    plt.show()\n","    \n","probs = lr.predict_proba(X_test_dtm)\n","probs = probs[:, 1]\n","auc = roc_auc_score(y_test, probs)\n","print('AUC: %.2f' % auc)\n","fpr, tpr, thresholds = roc_curve(y_test, probs)\n","plot_roc_curve(fpr, tpr)\n","\n","### KNN Model\n","\n","%%time\n","\n","knn_params = {'n_neighbors': np.arange(1,100,2)}\n","\n","knn = KNeighborsClassifier()\n","\n","knn_clf = GridSearchCV(knn, knn_params, verbose=5, n_jobs=-1)\n","knn_clf.fit(X_train_dtm, y_train)\n","knn_clf.best_params_\n","\n","knn = KNeighborsClassifier(n_neighbors=3)\n","\n","knn.fit(X_train_dtm, y_train)\n","\n","metrics.accuracy_score(y_test, knn.predict(X_test_dtm))\n","\n","# Print the confusion matrix\n","print(metrics.confusion_matrix(y_test, knn.predict(X_test_dtm)))\n","\n","# Print the precision and recall, among other metrics\n","print(metrics.classification_report(y_test, knn.predict(X_test_dtm), digits=3, target_names=target_names))\n","\n","## for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","classes = np.unique(y_test)\n","fig, ax = plt.subplots()\n","cm = metrics.confusion_matrix(y_test, knn.predict(X_test_dtm), labels=classes)\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n","ax.set_yticklabels(labels=classes, rotation=0)\n","plt.show()\n","\n","# roc curve and auc\n","\n","def plot_roc_curve(fpr, tpr):\n","    plt.plot(fpr, tpr, color='orange', label='ROC')\n","    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    plt.show()\n","    \n","probs = knn.predict_proba(X_test_dtm)\n","probs = probs[:, 1]\n","auc = roc_auc_score(y_test, probs)\n","print('AUC: %.2f' % auc)\n","fpr, tpr, thresholds = roc_curve(y_test, probs)\n","plot_roc_curve(fpr, tpr)\n","\n","### Naive Bayes Model\n","Commonly Used Model:<br>\n","https://itechindia.co/blog/which-of-the-3-algorithms-models-should-you-choose-for-sentiment-analysis-2/\n","\n","nb = MultinomialNB()\n","\n","nb.fit(X_train_dtm, y_train)\n","\n","metrics.accuracy_score(y_test, nb.predict(X_test_dtm))\n","\n","nb_params = {\n","    'alpha': [1000,500,100,50,10,7,6,5,4,2,1,0.5,0.1,0.05,0.01,0.005,0.001]\n","}\n","\n","clf_nb = GridSearchCV(nb, nb_params, verbose=5, n_jobs=-1)\n","clf_nb.fit(X_train_dtm, y_train)\n","\n","clf_nb.best_params_\n","\n","nb = MultinomialNB(alpha=0.5)\n","\n","nb.fit(X_train_dtm, y_train)\n","\n","metrics.accuracy_score(y_test, nb.predict(X_test_dtm))\n","\n","# Print the confusion matrix\n","print(metrics.confusion_matrix(y_test, nb.predict(X_test_dtm)))\n","\n","# Print the precision and recall, among other metrics\n","print(metrics.classification_report(y_test, nb.predict(X_test_dtm), digits=3, target_names=target_names))\n","\n","## for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","classes = np.unique(y_test)\n","fig, ax = plt.subplots()\n","cm = metrics.confusion_matrix(y_test, nb.predict(X_test_dtm), labels=classes)\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n","ax.set_yticklabels(labels=classes, rotation=0)\n","plt.show()\n","\n","# roc curve and auc\n","\n","def plot_roc_curve(fpr, tpr):\n","    plt.plot(fpr, tpr, color='orange', label='ROC')\n","    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    plt.show()\n","    \n","probs = nb.predict_proba(X_test_dtm)\n","probs = probs[:, 1]\n","auc = roc_auc_score(y_test, probs)\n","print('AUC: %.2f' % auc)\n","fpr, tpr, thresholds = roc_curve(y_test, probs)\n","plot_roc_curve(fpr, tpr)\n","\n","### Electra Model\n","Utilising pre-trained model using transformer model to learn the language\n","\n","# import os\n","# import pickle\n","\n","# import numpy as np\n","# import pandas as pd\n","# import tensorflow as tf\n","# import tensorflow.keras.layers as L\n","# from tensorflow.keras.optimizers import Adam\n","# from tensorflow.keras.models import Model\n","# from tensorflow.keras.callbacks import ModelCheckpoint\n","# from sklearn.model_selection import train_test_split\n","# from sklearn.metrics import classification_report, average_precision_score, roc_auc_score\n","# import matplotlib.pyplot as plt\n","# import transformers\n","# from transformers import AutoTokenizer, TFAutoModel, TFElectraModel, ElectraTokenizer\n","# from tqdm.notebook import tqdm\n","# from tokenizers import BertWordPieceTokenizer\n","\n","# # Import Electra Model\n","# import pickle\n","\n","# file = open(\"inputs/electra_model.pickle\", \"rb\")\n","# electra_model = pickle.load(file)\n","\n","# file = open(\"inputs/electra_tokenizer.pickle\", \"rb\")\n","# electra_tokenizer = pickle.load(file)\n","\n","# sentences = [\"there is a shortage of capital, and we need extra financing\", \n","#              \"growth is strong and we have plenty of liquidity\", \n","#              \"there are doubts about our finances\", \n","#              \"profits are flat\"]\n","\n","# inputs = electra_tokenizer(sentences, return_tensors=\"pt\", padding=True)\n","# outputs = electra_model(**inputs)[0]\n","\n","# labels = {0:'neutral', 1:'positive',2:'negative'}\n","# for idx, sent in enumerate(sentences):\n","#     print(sent, '----', labels[np.argmax(outputs.detach().numpy()[idx])])\n","\n","    \n","\n","### CNN Model\n","Reference: https://towardsdatascience.com/nlp-with-cnns-a6aa743bdc1e\n","\n","# from keras.layers.convolutional import Conv1D\n","# from keras.layers import GlobalMaxPooling1D, Dropout, Activation\n","\n","# # parameters\n","# batch_size = 32\n","# embedding_dims = 300\n","# filters = 250 # Length of the token vectors\n","# kernel_size = 3 # a window size of 3 tokens\n","# hidden_dims = 250 # number of neurons at the normal feedforward NN\n","# epochs = 2 \n","\n","# model = Sequential()\n","# model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1, input_shape=(X_train_dtm.shape[1], embedding_dims)))\n","# # model.add(GlobalMaxPooling1D()) # default = 2\n","# # model.add(Dense(hidden_dims))\n","# # model.add(Dropout(0.2))\n","# # model.add(Activation('relu'))\n","# # model.add(Dense(1))\n","# # model.add(Activation('sigmoid'))\n","# model.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = ['accuracy'])\n","# model.fit(X_train_dtm,y_train,batch_size = batch_size,epochs = epochs , validation_data = (X_test,y_test))\n","\n","### LSTM Neural Networks\n","\n","# model = Sequential()\n","# model.add(Dense(128, activation='relu', input_dim=X_train.shape[1]))\n","# model.add(Dense(64, activation='relu'))\n","# model.add(Dense(32, activation='relu'))\n","# model.add(Dense(8, activation='relu'))\n","# model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n","# model.fit(X_train_dtm, y_train, epochs=3)\n","\n","# # The maximum number of words to be used. (most frequent)\n","# MAX_NB_WORDS = 50000\n","# # Max number of words in each complaint.\n","# MAX_SEQUENCE_LENGTH = 250\n","# # This is fixed.\n","# EMBEDDING_DIM = 128\n","\n","# model = Sequential()\n","# model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=28))\n","# model.add(SpatialDropout1D(0.2))\n","# model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n","# model.add(Dense(2, activation='softmax'))\n","# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# epochs = 5\n","# batch_size = 64\n","\n","# history = model.fit(X_train_dtm, y_train, epochs=epochs, batch_size=batch_size)\n","\n","### FinBERT Model\n","\n","# # call the model\n","# from transformers import BertTokenizer, BertForSequenceClassification\n","# import pickle\n","# import numpy as np\n","\n","# # load tokenizer\n","# file_dir = 'inputs/finbert_tokenizer.pickle'\n","# file = open(f\"{file_dir}\", \"rb\")\n","# tokenizer = pickle.load(file)\n","\n","# # Load model\n","# file_dir = 'models/sentiment/'\n","# finbert = BertForSequenceClassification.from_pretrained(\n","#     f\"{file_dir}\", num_labels=3, local_files_only=True\n","# )\n","\n","# sentences = [\"there is a shortage of capital, and we need extra financing\", \n","#              \"growth is strong and we have plenty of liquidity\", \n","#              \"there are doubts about our finances\", \n","#              \"profits are flat\",\n","#              \"very happy excited economy\"]\n","\n","# inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n","# outputs = finbert(**inputs)[0]\n","\n","# labels = {0:'neutral', 1:'positive',2:'negative'}\n","# for idx, sent in enumerate(sentences):\n","#     print(sent, '----', labels[np.argmax(outputs.detach().numpy()[idx])])\n","\n","# # call the model\n","# from transformers import AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n","# import pickle\n","# import numpy as np\n","\n","# # load tokenizer\n","# file_dir = 'inputs/prosus_finbert_tokenizer.pickle'\n","# file = open(f\"{file_dir}\", \"rb\")\n","# tokenizer = pickle.load(file)\n","\n","# # Load model\n","# file_dir = 'models/sentiment/'\n","# finbert = BertForSequenceClassification.from_pretrained(\n","#     f\"{file_dir}\", num_labels = 3)\n","\n","# sentences = [\"there is a shortage of capital, and we need extra financing\", \n","#              \"growth is strong and we have plenty of liquidity\", \n","#              \"there are doubts about our finances\", \n","#              \"profits are flat\",\n","#             \"extremely positive economy outlook\"]\n","\n","# inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n","# outputs = finbert(**inputs)[0]\n","\n","# labels = {0:'neutral', 1:'positive',2:'negative'}\n","# for idx, sent in enumerate(sentences):\n","#     print(sent, '----', labels[np.argmax(outputs.detach().numpy()[idx])])\n","    \n","\n","# # prediction method for finbert model\n","\n","# labels = {0: 0, 1: 1, 2: -1}\n","\n","# def predict_fin(sentence):\n","#     \"\"\"\n","#     Using finbert model to predict the financial sentiment\n","#     \"\"\"\n","    \n","#     inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n","#     outputs = finbert(**inputs)[0]\n","    \n","#     outcome = labels[np.argmax(outputs.detach().numpy()[idx])]\n","    \n","#     return outcome\n","\n","# # predict df of the news article\n","\n","# df['Predict'] = df['cleaned_headline'].apply(lambda x: predict_fin(x))\n","# df\n","\n","### Ensemble Model\n","\n","base_models = [\n","    ('xg', xg),\n","    ('svm', sv),\n","    ('lr', lr),\n","    ('knn', knn),\n","    ('nb', nb)\n","]\n","\n","meta_model = svm.SVC(C=100, gamma=1, probability=True)\n","\n","%%time\n","from sklearn.ensemble import StackingClassifier\n","\n","stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, n_jobs=-1, verbose=5)\n","stacked_model.fit(X_train_dtm, y_train)\n","stacked_model.score(X_test_dtm, y_test)\n","\n","%%time\n","\n","stacked_model_svm = StackingClassifier(estimators=base_models, final_estimator=lr, n_jobs=-1, verbose=5)\n","stacked_model_svm.fit(X_train_dtm, y_train)\n","stacked_model_svm.score(X_test_dtm, y_test)\n","\n","# Print the confusion matrix\n","print(metrics.confusion_matrix(y_test, stacked_model_svm.predict(X_test_dtm)))\n","\n","# Print the precision and recall, among other metrics\n","print(metrics.classification_report(y_test, stacked_model_svm.predict(X_test_dtm), digits=3, target_names=target_names))\n","\n","## for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","classes = np.unique(y_test)\n","fig, ax = plt.subplots()\n","cm = metrics.confusion_matrix(y_test, stacked_model_svm.predict(X_test_dtm), labels=classes)\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n","ax.set_yticklabels(labels=classes, rotation=0)\n","plt.show()\n","\n","# roc curve and auc\n","\n","def plot_roc_curve(fpr, tpr):\n","    plt.plot(fpr, tpr, color='orange', label='ROC')\n","    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    plt.show()\n","    \n","probs = stacked_model_svm.predict_proba(X_test_dtm)\n","probs = probs[:, 1]\n","auc = roc_auc_score(y_test, probs)\n","print('AUC: %.2f' % auc)\n","fpr, tpr, thresholds = roc_curve(y_test, probs)\n","plot_roc_curve(fpr, tpr)\n","\n","### FinBERT Model\n","\n","# load in finbert model\n","import pickle\n","\n","f = open('outputs/finbert.pickle', \"rb\")\n","finbert = pickle.load(f)\n","\n","f = open('outputs/tokenizer.pickle', \"rb\")\n","tokenizer = pickle.load(f)\n","\n","import numpy as np\n","\n","# {0: 'neutral', 1: 'positive', 2: 'negative'}\n","labels = {0: 0, 1: 1, 2: 0}\n","\n","def label_sentiment(sentence):  # take in list sentence as input\n","    \"\"\"\n","    Transfer learning using FinBERT to train the model\n","    Note: sentences cannot be an empty list, else function will not work\n","    \"\"\"\n","    assert len(sentence) != 0, \"Sentences is an empty list, please debug to ensure that empty sentences are removed in df\"\n","\n","    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True)\n","    outputs = finbert(**inputs)[0]\n","    \n","    val = labels[np.argmax(outputs.detach().numpy())]\n","    \n","    return val\n","\n","import copy\n","\n","fin_df = df.copy()\n","\n","# Long computation (slower than other models) --> However, when productionalising, it is much faster.\n","\n","fin_df['fin_label'] = fin_df['cleaned_headline'].apply(lambda x: label_sentiment(x)) # prediction method for finbert\n","\n","from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, f1_score\n","\n","print(f\"Accuracy: {accuracy_score(fin_df['class_label'], fin_df['fin_label'])}\")\n","print(f\"Precision: {precision_score(fin_df['class_label'], fin_df['fin_label'])}\")\n","print(f\"ROC AUC: {roc_auc_score(fin_df['class_label'], fin_df['fin_label'])}\")\n","print(f\"F1 Score: {f1_score(fin_df['class_label'], fin_df['fin_label'])}\")\n","\n","### Model Evaluation\n","Summary of all the models tested\n","\n","def model_performance(models, model_names):\n","    #create empty df with col names\n","    df = pd.DataFrame(columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'ROC AUC'])\n","\n","    for n, model in enumerate(models):\n","        \n","        if model_names[n] == 'finbert':\n","            name = model_names[n]\n","\n","            acc = metrics.accuracy_score(fin_df['class_label'], fin_df['fin_label'])\n","            prec = metrics.precision_score(fin_df['class_label'], fin_df['fin_label'])\n","            recall = metrics.recall_score(fin_df['class_label'], fin_df['fin_label'])\n","            f1 = metrics.f1_score(fin_df['class_label'], fin_df['fin_label'])\n","            roc_auc = metrics.roc_auc_score(fin_df['class_label'], fin_df['fin_label'])\n","            \n","        else:\n","        \n","            y_pred = model.predict(X_test_dtm)\n","\n","            name = model_names[n]\n","\n","            acc = metrics.accuracy_score(y_test, y_pred)\n","            prec = metrics.precision_score(y_test, y_pred)\n","            recall = metrics.recall_score(y_test, y_pred)\n","            f1 = metrics.f1_score(y_test, y_pred)\n","            roc_auc = metrics.roc_auc_score(y_test, y_pred)\n","\n","        #append row to df\n","        df = df.append(\n","            {\n","                'Model' : name,\n","                'Accuracy': acc,\n","                'Precision': prec,\n","                'Recall': recall,\n","                'F1 score': f1,\n","                'ROC AUC': roc_auc\n","            }, ignore_index = True)\n","\n","    return df.set_index('Model').transpose()\n","\n","model_performance([finbert, xg, sv, lr, knn, nb, stacked_model], ['finbert', 'xg', 'svm', 'lr', 'knn', 'nb', 'stack'])\n","\n","### Finalised Model\n","\n","# # save model to pickle\n","# import pickle\n","\n","# # save the model to disk\n","# filename = 'outputs/finalized_model_tuned.pickle'\n","# dbfile =  open(filename, 'wb')\n","# pickle.dump(stacked_model, dbfile)\n","# dbfile.close()\n","\n","# # save the model to disk\n","# filename = 'outputs/tfidf_vect_tuned.pickle'\n","# dbfile =  open(filename, 'wb')\n","# pickle.dump(tfidf_vect, dbfile)\n","# dbfile.close()"]}]}
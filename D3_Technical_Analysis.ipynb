{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C_03_Production_Stage_New_Feature_Technical_Analysis.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM5fOshbL38F2wM+mjidnhn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"6GqLgIMn7unm"},"outputs":[],"source":["## Production Stage (With New Feature Engineering)\n","Use our ensembled model to classify and label the dataset for the AI/ML model. Combining ESS-Grouping feature engineering, this production process serves to provide robust signals to the optimization process.\n","\n","import pandas as pd\n","import numpy as np\n","import string\n","import re\n","import pickle\n","import matplotlib.pyplot as plt\n","import ta\n","\n","from sklearn.preprocessing import LabelEncoder\n","%matplotlib inline\n","\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)\n","pd.set_option('display.width', 1000)\n","\n","%run import_library.ipynb\n","\n","### Import RavenPack ID\n","Import SEDOL and RavenPack ID for the china financial sector\n","\n","rp_df = pd.read_excel('inputs/RavenPack_ID_Names.xlsx')\n","rp_df = standardize_col_names(rp_df)\n","rp_df = rp_df[rp_df['rp_entity_id'].notnull()].reset_index(drop=True) # remove datasets with no ravenpack id\n","\n","# only filter for HK Equity due to data constraints (not enough data points) --> therefore 1 to 1 match for rp_entity_id & sedol\n","rp_df = rp_df[rp_df['cdr_country'].str.contains(r'HK Equity')]\n","rp_df.reset_index(inplace=True, drop=True)\n","rp_df\n","\n","# import labelled dataset from pre-labelled data in RavenPack\n","import datetime\n","\n","batch_id = datetime.date.today().strftime(\"%y%m%d\")\n","df = pd.read_csv(f'outputs/china_news_sentiments.csv', encoding='latin-1')\n","df = df[['timestamp_tz', 'cleaned_headline', 'class', 'entity_name', 'rp_entity_id', 'group', 'event_sentiment_score']]\n","df = df[df['cleaned_headline'].notna()]\n","df.reset_index(inplace=True, drop=True)\n","df\n","\n","### Predict Historical Dataset\n","Machine learning model on NLP Documents\n","\n","# FOR MACHINE LEARNING METHOD - Sentiment Analysis\n","\n","# remove rows with null values for cleaned_headline\n","df['cleaned_headline'].replace('', np.nan, inplace=True)\n","df.dropna(subset=['cleaned_headline'], inplace=True)\n","df.reset_index(drop=True, inplace=True)\n","\n","le = LabelEncoder()\n","df['class_label'] = le.fit_transform(df['class'])\n","\n","# Vectorise Corpus\n","X_final_dtm = tfidf_vect.transform(df.cleaned_headline)\n","\n","# Model Prediction\n","result = stacked_model_svm.predict(X_final_dtm)\n","df['model_score'] = result\n","df\n","\n","## Feature Engineering for Event Group\n","- We will be creating specific ESS scores for certain important events based on the extreme values of the ESS.\n","- For instance, we will be assessing the top 10 and bottom 10 in terms of the average ESS scores over the entire 20 year duration.\n","\n","df = event_feature_engineering(df)\n","df\n","\n","### Productionalize the Companies\n","Conduct by entities name for individual firms. Note: consist of 83 RavenPack ID, with 96 Entity Names, ie, 1 RavenPack ID can have multiple entity names. By filtering for only Hong Kong listed equities, we seek to conduct sentiment analysis on these to generalize for financials markets, to obtain a buy hold decision for the sector.\n","\n","# use only rp hk listed entities\n","rp_entity_id_list = list(rp_df.rp_entity_id.unique())\n","len(rp_entity_id_list)\n","\n","import copy\n","# conduct monthly aggregation of dataframe of china financial stocks for sentiment analysis\n","\n","df1 = df.copy()\n","ldf = []\n","\n","for i in rp_entity_id_list:    \n","    \n","    # segregating into individual companies\n","    df = df1[(df1['rp_entity_id'] == i)]\n","    df['timestamp_tz'] = pd.to_datetime(df.timestamp_tz) # set the date based on yymmdd\n","    df.rename(columns={'timestamp_tz': 'date'}, inplace=True)\n","    df = df.set_index('date')\n","    df.sort_index(ascending=True, inplace=True)\n","    df = df[['event_sentiment_score', 'model_score', 'mergers_event_sentiment_score', 'mergers_model_score', 'product_event_sentiment_score', 'product_model_score', 'earnings_event_sentiment_score', 'earnings_model_score', 'ratings_event_sentiment_score', 'ratings_model_score', 'regulatory_event_sentiment_score', 'regulatory_model_score']]\n","    \n","    # monthly aggregation of the data points\n","    df = aggregate_period(df, 'M', 'mean')\n","    \n","    # conduct percentile ranking\n","    df = event_percentile_rank(df)\n","    df['rp_entity_id'] = i # to reinstantiate id value\n","    ldf.append(df)\n","    \n","sentiment_df = pd.concat(ldf, axis=0)\n","sentiment_df.reset_index(inplace=True)\n","sentiment_df = sentiment_df.merge(rp_df[['sedol', 'rp_entity_id']], on='rp_entity_id', how='right')\n","sentiment_df = sentiment_df.set_index('date') # set the date\n","sentiment_df\n","\n","## Import Price Data\n","Import price and volume for technical analysis\n","\n","price_df = pd.read_csv(\"C:/Users/tmp4lv/Documents/uCloud/px/px_vol_mktcap.csv\", index_col=False)\n","price_df = standardize_col_names(price_df)\n","price_df['date'] = pd.to_datetime(price_df.date)\n","price_df = price_df[(price_df.date >= \"2000-01-01\")]\n","price_df.reset_index(inplace=True, drop=True)\n","price_df['sedol'] = price_df['sedol'].astype(str) # to cast the column as strings to prevent mismatch with rp_df\n","price_df\n","\n","Note: Some RavenPack ID represents multiple SEDOL, meaning an RP ID can represents multiple SEDOL of the same company name.\n","\n","price_df = price_df.merge(rp_df[['sedol', 'rp_entity_id']], on='sedol', how='right')\n","price_df = price_df[price_df['px'].notnull()].reset_index(drop=True) # ignore data points that do not have values\n","price_df = price_df.set_index('date') # set index\n","\n","# remove rows with 0 volume in df - mostly due to it being chinese holiday hence lack of data\n","price_df = price_df[price_df['vol']!=0]\n","price_df\n","\n","### Technical Analysis (Momentum Indicator)\n","Conducting potential technical analysis on pricing data.\n","1. Relative Strength Index (RSI)\n","\n","### Relative Strength Index (RSI)\n","RSI is a technical indicator, and is intended to chart the current and historical strength or weakness of a stock or market based on the closing prices of a recent trading period. It compares the magnitude of recent gains and losses over a specified time period to measure speed and change of price movements of a security. In short, it generates overbought or oversold signals. Good for stable periods with minimal disruptions. Values above 70 == overbought or overvalued (slide below 70 means bearish). Values below 30 == oversold or undervalued (bullish signals). Feature Engineering: below 30 --> potential buy signal, above 70 --> potential sell signal\n","\n","### Technical Analysis (Volume Indicator)\n","Conducting potential technical analysis on pricing data.\n","1. Ease of movement\n","\n","### Ease of Movement\n","EVM is a volume based oscillator, indicating the ease with which the prices rise or fall taking into account the volume of the security. Example, price rise on low volume means prices advanced with relative ease, indicating little selling pressure. Positive EVM values imply that the market is moving higher with ease, while negative values indicate an easy decline. Purpose: used to confirm bullish or bearish trend. Increase in price with positive EVM confirms bullish trend, decrease in price with negative EVM confirms bearish trend.\n","\n","### Technical Analysis (Volatility Indicator)\n","Conducting potential technical analysis on pricing data.\n","1. Bollinger Bands\n","\n","### Bollinger Bands\n","Bollinger bands are often used to determine overbought and oversold conditions. Indicator focus on price and volatility (could be too biased). Rules: when the price breaks below the band, tend to bounce up, hence it is a buy strategy. when price breaks above the upper band, overbought and due for a pullback. Related to a mean reversion concept of price. FYI, the bands adapt to price expanding and contracting as volatility increases and decreases.\n","\n","### Technical Analysis (Trend Indicator)\n","Conducting potential technical analysis on pricing data.\n","1. Average Directional Movement Index\n","2. Moving Average Convergence Divergence (MACD)\n","\n","### Average Directional Movement Index (Trend Indicator)\n","ADX measure the strength of a trend. The higher the magnitude of ADX, the stronger the trend.\n","\n","### Moving Average Convergence Divergence (MACD)\n","MACD shows the relationship between two exponential moving averages of a stock price. Comparing MACD line against signal line (ie, 9-day EMA). MACD Diff indicates that if the value is positive, it signals a bullish outlook and positive momentum. Else, negative indicates bearish outlook and negative momentum.\n","\n","sedol_list = list(rp_df.sedol.unique())\n","\n","ldf = []\n","for i in sedol_list:    \n","    \n","    # segregating into individual companies\n","    df = price_df[(price_df['sedol'] == i)]\n","    df = df.sort_index(ascending=True) # sort according to the dates\n","    \n","    for j in [5, 14, 50]:\n","        \n","        # rsi\n","        df[f'rsi_{j}'] = ta.momentum.rsi(df['px'], j)\n","        df[f'rsi_{j}'] = df[f'rsi_{j}'].apply(lambda x: 1 if x < 30 else 0)\n","        \n","        # ease of movement\n","        df[f'evm_{j}'] = ta.volume.ease_of_movement(df['px_high'], df['px_low'], df['vol'], j)\n","        \n","        # bollinger bands\n","        # bollinger high band - indicator shows if the band has been surpassed\n","        df[f'bol_hband_{j}'] = ta.volatility.bollinger_hband_indicator(df['px'], j)\n","\n","        # bollinger low band - indicator shows if the band has been surpassed\n","        df[f'bol_lband_{j}'] = ta.volatility.bollinger_lband_indicator(df['px'], j)\n","\n","        # bollinger band width - indicates volatility (falling --> lower volatility, increasing --> higher volatility)\n","        df[f'bol_wband_{j}'] = ta.volatility.bollinger_wband(df['px'], j)\n","\n","        # buy signals generated from bollinger bands rules, 1 mean buy, 0 means hold\n","        df[f'bol_buy_{j}'] = df[f'bol_lband_{j}'].apply(lambda x: 1 if x > 0 else 0)\n","        \n","        # adx\n","        df[f'adx_{j}'] = ta.trend.adx(df['px_high'], df['px_low'], df['px'], window=j)\n","\n","    # macd\n","    df[f'macd_12_26'] = ta.trend.macd_diff(df['px'])  \n","\n","    ldf.append(df)\n","    \n","price_df = pd.concat(ldf, axis=0)\n","price_df\n","\n","### Visualisations for Price Dataset\n","\n","# visualise sample price dataset\n","\n","temp_df = price_df[price_df['sedol'] == \"B154564\"]\n","\n","\n","import plotly.graph_objects as go\n","\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(y=temp_df[\"px\"], x=temp_df.index, name=\"msci\"))\n","fig.update_layout(\n","    hovermode='x',\n","    title=f\"Daily Time Series for B154564\",\n","    xaxis_title=\"Time Period\",\n","    yaxis_title=\"Price\",\n","    autosize=True,\n",")\n","fig.update_xaxes(rangeslider_visible=True, showgrid=True, gridwidth=1, gridcolor='#ECECEC', zeroline=True, zerolinecolor='lightgrey')\n","fig.update_yaxes(automargin=True)\n","\n","### Calculate Periodic Returns + Combine with Technical Indicators\n","As RP_ENTITY_ID can represents multiple SEDOL, hence we focus on the SEDOL instead of RP_ENTITY_ID when combined with price and volume datasets. To calculate periodic returns:\n","- Do a preiodic pricing of the returns (use the last of the daily return for the month)\n","- Use mean of average trading volume\n","\n","# conduct monthly aggregation of dataframe of china financial stocks for returns and volume averaging\n","# join with technical indicators as well\n","\n","ldf = []\n","for i in sedol_list:    \n","    \n","    # segregating into individual companies\n","    price_df_filtered = price_df[(price_df['sedol'] == i)]\n","    \n","    if price_df_filtered.empty:\n","        print(f\"=== Sedol {i} is empty ====\")\n","    \n","    df = price_df_filtered.copy().sort_index(ascending=True)\n","    \n","    sentiment_df_filtered = sentiment_df[(sentiment_df['sedol'] == i)]\n","    sentiment_df_filtered.drop(['sedol'], axis=1, inplace=True) # for merging and prevent duplicated sedol\n","    \n","    if sentiment_df_filtered.empty:\n","        print(f\"=== Sedol {i} is empty for sentiment df ====\")\n","    \n","    df_month_returns = aggregate_period(df[['px']], 'M', 'last')\n","    df_month_returns['returns'] = df_month_returns['px']/df_month_returns['px'].shift(1) - 1\n","    df_month_returns.dropna(subset=['returns'], inplace=True)\n","    df_month_returns['returns_lead_1'] = df_month_returns['returns'].shift(-1)\n","    \n","    df_month_returns['sedol'] = i # assign sedol, to allow the merge function to work even without temp_sentiment\n","    df_month_vol = aggregate_period(df[['vol']], 'M', 'mean')\n","    \n","    # merge on right to keep close to the price and vol, instead of using sentiment as a base case\n","    df = df_month_returns.merge(df_month_vol, left_index=True, right_index=True, how='left').merge(sentiment_df_filtered, left_index=True, right_index=True, how='left')\n","    \n","    # imputation - mainly for new data points (not sure if its good)\n","    df = df.ffill(axis=0).bfill(axis=0)\n","    \n","    \n","    # for technical indicators (ie, price_df)\n","    ldf_2 = []\n","\n","    for i in [5, 14, 50]:\n","\n","        # sum for binary variables\n","        df_sum = aggregate_period(price_df_filtered[[f'rsi_{i}', f'evm_{i}', f'bol_buy_{i}']], 'M', 'sum')\n","\n","        # mean for continuous variables\n","        df_mean = aggregate_period(price_df_filtered[[f'bol_wband_{i}', f'adx_{i}']], 'M', 'mean')\n","\n","        ldf_2.append(df_sum)\n","        ldf_2.append(df_mean)\n","\n","    # mean for macd variable\n","    df_macd_mean = aggregate_period(price_df_filtered[[f'macd_12_26']], 'M', 'mean')\n","    ldf_2.append(df_macd_mean)\n","\n","    final_df = pd.concat(ldf_2, axis=1)\n","    df = df.merge(final_df, left_index=True, right_index=True, how='left')\n","    \n","    ldf.append(df)\n","    \n","china_df = pd.concat(ldf, axis=0)\n","china_df\n","\n","# check for any nan values\n","china_df[china_df.isnull().any(1)].sort_values('date')\n","\n","# sedol list\n","ldf = []\n","\n","for i in sedol_list:\n","    \n","    macd_df = macd_feature_engineering(china_df, i)\n","    ldf.append(macd_df)\n","    \n","china_df = pd.concat(ldf, axis=0)\n","china_df \n","\n","### Combine Datasets With News Volume Spikes\n","\n","df_news_vol_spikes =  pd.read_csv(\"outputs/china_news_volume_spikes.csv\", encoding='latin-1')\n","df_news_vol_spikes['date'] = pd.to_datetime(df_news_vol_spikes.date) # convert to datetime objects\n","df_news_vol_spikes.set_index('date', inplace=True) # set index\n","df_news_vol_spikes\n","\n","# left join with existing dataframe\n","\n","ldf = []\n","for i in rp_entity_id_list:    \n","    \n","    # segregating into individual companies\n","    temp_china_df = china_df[(china_df['rp_entity_id'] == i)]\n","    \n","    temp_df_news_vol_spikes = df_news_vol_spikes[(df_news_vol_spikes['rp_entity_id'] == i)]\n","    df = temp_china_df.merge(temp_df_news_vol_spikes.drop(['rp_entity_id'], axis=1), left_index=True, right_index=True, how='left')\n","    \n","    ldf.append(df)\n","    \n","final_df = pd.concat(ldf, axis=0)\n","final_df\n","\n","### Macroeconomics Variables\n","Use China Data\n","\n","macro_df = pd.read_csv('inputs/country_macro.csv')\n","cn_df = macro_df[(macro_df['MSCI_COUNTRY'] == \"China\")] # not used at the moment\n","\n","# formatting: convert to datetime objects\n","cn_df['Periods'] = pd.to_datetime(cn_df.Periods)\n","cn_df.sort_values(by=['Periods'], ascending=True, inplace=True)\n","cn_df.reset_index(inplace=True, drop=True)\n","cn_df = cn_df[['Periods', 'GDP_GROWTH', 'CPI_GROWTH']] # remove unemployment due to no data\n","cn_df.rename(columns={'GDP_GROWTH': \"gdp_growth_cn\", 'CPI_GROWTH': 'cpi_growth_cn', \"Periods\": \"date\"}, inplace=True)\n","cn_df = cn_df.ffill()\n","cn_df.set_index('date', inplace=True, drop=True)\n","cn_df.index = cn_df.index + MonthEnd(0)\n","\n","# us macro dataframe\n","us_df = macro_df[(macro_df['MSCI_COUNTRY'] == \"United States\")] # used\n","\n","# formatting: convert to datetime objects\n","us_df['Periods'] = pd.to_datetime(us_df.Periods)\n","us_df.sort_values(by=['Periods'], ascending=True, inplace=True)\n","us_df.reset_index(inplace=True, drop=True)\n","us_df = us_df[['Periods', 'GDP_GROWTH', 'CPI_GROWTH', 'UNEMPLOYMENT_RATE']]\n","us_df.rename(columns={'GDP_GROWTH': \"gdp_growth_us\", 'CPI_GROWTH': 'cpi_growth_us', \"UNEMPLOYMENT_RATE\": 'unemployment_rate_us', \"Periods\": \"date\"}, inplace=True)\n","us_df = us_df.ffill()\n","us_df.set_index('date', inplace=True, drop=True)\n","us_df.index = us_df.index + MonthEnd(0)\n","\n","# Merge with CN Market\n","\n","final_df = final_df.merge(cn_df, left_index=True, right_index=True)\n","final_df = final_df.merge(us_df, left_index=True, right_index=True)\n","\n","print(f\"=== Before: Checking for Null Values: {len(final_df[final_df.isnull().any(1)].sort_values('date'))} qty of null values ===\")\n","final_df.dropna(subset=['cpi_growth_cn'], inplace=True) # drop nan values from china cpi data\n","print(f\"=== After: Checking for Null Values: {len(final_df[final_df.isnull().any(1)].sort_values('date'))} qty of null values ===\")\n","final_df\n","\n","### Save to pickle dataframes files\n","\n","# # save model to pickle\n","# import pickle\n","\n","# # save the model to disk\n","# filename = 'outputs/finalised_df/df_cn_month.pickle'\n","# dbfile =  open(filename, 'wb')\n","# pickle.dump(final_df, dbfile)\n","# dbfile.close()\n","\n","# End of Production Codes"]}]}
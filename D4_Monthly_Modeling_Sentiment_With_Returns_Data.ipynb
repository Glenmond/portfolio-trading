{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"C_04_Monthly_Modeling_Sentiment_With_Returns_Data_ESS_Features.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNmQ8PzjDI/3NDaDCpdsLKJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zMgypvel8Srb"},"outputs":[],"source":["# Classification Analysis (Monthly)\n","We will model the returns movement using technical and sentiment analysis. Technical analysis is the method of using statistical methods and trends based on historical data, such as daily total volume or value of a traded stock, and evaluate the historical patterns to predict future stock price movement. On the other hand, sentiment analysis utilises textual data to predict the sentiment direction that influences stock price movements. We utilise self-engineered features from ESS scores. Reference: https://medium.com/codex/stock-predication-using-regression-algorithm-in-python-fb8b426453b9\n","\n","import pandas as pd\n","import pickle\n","import numpy as np\n","import math \n","import seaborn as sns\n","import statsmodels.api as sm\n","import matplotlib.pyplot as plt\n","\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import svm, tree\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.ensemble import RandomForestClassifier\n","from pandas.tseries.offsets import MonthEnd\n","%matplotlib inline\n","plt.style.use('fivethirtyeight')\n","\n","%run import_library.ipynb\n","\n","file = open('outputs/finalised_df/df_cn_month.pickle', \"rb\")\n","df = pickle.load(file)\n","df\n","\n","# for feature selection for respective Ml models\n","\n","feature_select_dict = {'xgb': [], 'svm_clf': [], 'sgd': [], 'bnb': [], 'rf': []} # dictionary of feature selection for different models\n","\n","## Multi-Collinearity Detection\n","We want to check if there are presence of multi-collinearity among independent variables. Reference: https://www.analyticsvidhya.com/blog/2021/03/multicollinearity-in-data-science/\n","\n","### Variance Inflation Factor (VIF)\n","\n","names = X_clf # classification variables\n","\n","import copy\n","\n","vif_df = df.copy()\n","vif_df = vif_df[names]\n","high_collinearity = []\n","\n","for index in range(0, len(names)):\n","    \n","    y = vif_df.loc[:, vif_df.columns == names[index]]\n","    x = vif_df.loc[:, vif_df.columns != names[index]]\n","    model = sm.OLS(y, x) #Fit ordinary least squares method\n","    results = model.fit()\n","    rsq = results.rsquared\n","    vif = round(1 / (1 - rsq), 2)\n","    print(\"R Square value of {} column is {} keeping all other columns as independent features\".format(\n","      names[index], (round(rsq, 2))\n","            )\n","    )\n","    print(\"Variance Inflation Factor of {} column is {} n\".format(\n","         names[index], vif)\n","    )\n","    print()\n","    \n","    if vif >=5: # high multicollinearity\n","        high_collinearity.append(names[index])\n","\n","high_collinearity\n","\n","# features to remove (manual basis) - for technical indicators, we will be using mid_window\n","\n","remove_features = ['model_score', 'rsi_5', 'rsi_50', 'evm_5', 'evm_50', 'bol_buy_5', 'bol_buy_50', 'bol_wband_5', 'bol_wband_50', 'adx_5', 'adx_50']\n","remove_features\n","\n","# Monthly Modelling\n","\n","## Train Test Split\n","\n","MinMaxScaler to encode numerical variables\n","\n","def returns_movement(s):\n","    \"\"\"\n","    Indicates the individual returns from historical data one period ahead, individual holdings returns must be more than others returns\n","    \"\"\"\n","    \n","    if (s['returns_lead_1'] > s['others_returns_lead_1']):\n","        return 1\n","    \n","    else:\n","        return 0\n","\n","df['returns_movement'] = df.apply(lambda x: returns_movement(x), axis=1) # y true for the next week data\n","\n","X_train, y_train, X_test, y_test = _train_test_split(df, list(df.sedol.unique()), \"2019-01-01\")\n","\n","# Scaling and Transformation\n","min_max_scaler = MinMaxScaler()\n","X_train[X_clf] = min_max_scaler.fit_transform(X_train[X_clf])\n","X_test[X_clf] = min_max_scaler.transform(X_test[X_clf])\n","\n","df.returns_movement.value_counts()\n","\n","### Stepwise Regression\n","To eliminate multilcollinearity for certain models, we seek to drop highly correlated variables\n","\n","import pandas as pd\n","import statsmodels.api as sm\n","\n","def forward_regression(X, y,\n","                       threshold_in=0.01,\n","                       verbose=False):\n","    initial_list = []\n","    included = list(initial_list)\n","    while True:\n","        changed=False\n","        excluded = list(set(X.columns)-set(included))\n","        new_pval = pd.Series(index=excluded)\n","        for new_column in excluded:\n","            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n","            new_pval[new_column] = model.pvalues[new_column]\n","        best_pval = new_pval.min()\n","        if best_pval < threshold_in:\n","            best_feature = new_pval.idxmin()\n","            included.append(best_feature)\n","            changed=True\n","            if verbose:\n","                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n","\n","        if not changed:\n","            break\n","\n","    return included\n","\n","def backward_regression(X, y,\n","                           threshold_out=0.05,\n","                           verbose=False):\n","    included=list(X.columns)\n","    while True:\n","        changed=False\n","        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n","        # use all coefs except intercept\n","        pvalues = model.pvalues.iloc[1:]\n","        worst_pval = pvalues.max() # null if pvalues is empty\n","        if worst_pval > threshold_out:\n","            changed=True\n","            worst_feature = pvalues.idxmax()\n","            included.remove(worst_feature)\n","            if verbose:\n","                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n","        if not changed:\n","            break\n","    return included\n","\n","forward_regression(X_train, y_train)\n","\n","backward_regression(X_train, y_train)\n","\n","## XGBoost Classifier\n","\n","import numpy as np\n","from xgboost import XGBRegressor\n","from sklearn.metrics import mean_squared_error as MSE\n","from itertools import product\n","from sklearn.preprocessing import LabelEncoder\n","\n","#import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from xgboost import XGBRegressor\n","from xgboost import plot_importance\n","\n","from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n","from sklearn import svm\n","from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n","from sklearn.kernel_ridge import KernelRidge\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n","from sklearn.model_selection import KFold, cross_val_score, train_test_split\n","from sklearn.metrics import mean_squared_error\n","import xgboost as xgb\n","\n","def plot_features(booster, figsize):    \n","    fig, ax = plt.subplots(1,1,figsize=figsize)\n","    return plot_importance(booster=booster, ax=ax)\n","\n","xg = XGBClassifier()\n","xg.fit(X_train, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_train, xg.predict(X_train))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_train, xg.predict(X_train))}\")\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, xg.predict(X_test))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, xg.predict(X_test))}\")\n","\n","%%time\n","\n","xg_params = {'max_depth': [3,6,9],\n","           'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 1.0],\n","           'n_estimators': [10, 50, 100, 200, 300],\n","           'colsample_bytree': [0.3, 0.7, 1.0, 1.3],\n","            'min_child_weight': range(1,10,2),\n","            'gamma': [0, 2, 4, 6, 8, 10]} #2,3 might not want these to prevent overfitting\n","\n","# xg_clf = GridSearchCV(xg, xg_params, scoring='precision', verbose=5, n_jobs=-1, cv=5)\n","xg_clf = GridSearchCV(xg, xg_params, scoring='precision', verbose=5, n_jobs=-1, cv=3)\n","xg_clf.fit(X_train, y_train)\n","\n","xg_clf.best_params_\n","\n","Best parameter for xgboost classification:\n","{'colsample_bytree': 1.0,\n"," 'gamma': 10,\n"," 'learning_rate': 0.05,\n"," 'max_depth': 6,\n"," 'min_child_weight': 3,\n"," 'n_estimators': 10}\n","\n","xg= XGBClassifier(**xg_clf.best_params_)\n","xg.fit(X_train, y_train)\n","\n","# train\n","print(f\"Train Accuracy Score: {metrics.accuracy_score(y_train, xg.predict(X_train))}\")\n","print(f\"Train Precision Score: {metrics.precision_score(y_train, xg.predict(X_train))}\")\n","\n","# test\n","print(f\"Test Accuracy Score: {metrics.accuracy_score(y_test, xg.predict(X_test))}\")\n","print(f\"Test Precision Score: {metrics.precision_score(y_test, xg.predict(X_test))}\")\n","\n","# manual input based on previous gridsearch cv\n","xg = XGBClassifier(**{'colsample_bytree': 1.0,\n"," 'learning_rate': 0.05,\n"," 'max_depth': 6,\n"," 'min_child_weight': 3,\n"," 'n_estimators': 10,\n"," 'gamma':10})\n","xg.fit(X_train, y_train)\n","\n","# train\n","print(f\"Train Accuracy Score: {metrics.accuracy_score(y_train, xg.predict(X_train))}\")\n","print(f\"Train Precision Score: {metrics.precision_score(y_train, xg.predict(X_train))}\")\n","\n","# test\n","print(f\"Test Accuracy Score: {metrics.accuracy_score(y_test, xg.predict(X_test))}\")\n","print(f\"Test Precision Score: {metrics.precision_score(y_test, xg.predict(X_test))}\")\n","\n","pd.DataFrame(list(xg.get_booster().get_score(importance_type='gain').items())).sort_values(1,ascending=False)\n","\n","### Selecting threshold based on precision score\n","\n","from sklearn.feature_selection import SelectFromModel\n","from numpy import sort, array\n","from sklearn.metrics import precision_score\n","\n","# Fit model using each importance as a threshold\n","xg_clf_tuned = XGBClassifier(**{'colsample_bytree': 1.0,\n"," 'learning_rate': 0.05,\n"," 'max_depth': 6,\n"," 'min_child_weight': 3,\n"," 'n_estimators': 10,\n"," 'gamma':10})\n","xg_clf_tuned.fit(X_train, y_train)\n","thresholds = sort(xg_clf_tuned.feature_importances_)\n","\n","xg_final_params = {'threshold': 0, 'precision': 0, 'num_features': 0, 'columns': []}\n","\n","for thresh in thresholds:\n","    \n","    # select features using threshold\n","    selection = SelectFromModel(xg_clf_tuned, threshold=thresh, prefit=True)\n","    select_X_train = selection.transform(X_train)\n","    \n","    # train model\n","    selection_model = XGBClassifier(**{'colsample_bytree': 1.0,\n","                        'learning_rate': 0.05,\n","                        'max_depth': 6,\n","                        'min_child_weight': 3,\n","                        'n_estimators': 10,\n","                        'gamma':10})\n","\n","    selection_model.fit(select_X_train, y_train)\n","    \n","    # eval model\n","    select_X_test = selection.transform(X_test)\n","    predictions = selection_model.predict(select_X_test)\n","    precision = precision_score(y_test, predictions)\n","    print(\"Thresh=%.3f, n=%d, Precision: %.2f%%\" % (thresh, select_X_train.shape[1], precision*100.0))\n","    num_features = select_X_train.shape[1]\n","    \n","    if precision > xg_final_params['precision'] and num_features >= 5: # must have at least 5 features\n","        xg_final_params[\"threshold\"] = thresh\n","        xg_final_params[\"precision\"] = precision\n","        xg_final_params[\"num_features\"] = num_features\n","        xg_final_params[\"columns\"] = X_train.columns[selection.get_support()].tolist()\n","\n","# from sklearn.feature_selection import SelectFromModel\n","# from numpy import sort, array\n","# from sklearn.metrics import precision_score\n","\n","# # Fit model using each importance as a threshold\n","# xg_clf_tuned = XGBClassifier(**xg_clf.best_params_)\n","# xg_clf_tuned.fit(X_train, y_train)\n","# thresholds = sort(xg_clf_tuned.feature_importances_)\n","\n","# xg_final_params = {'threshold': 0, 'precision': 0, 'num_features': 0, 'columns': []}\n","\n","# for thresh in thresholds:\n","    \n","#     # select features using threshold\n","#     selection = SelectFromModel(xg_clf_tuned, threshold=thresh, prefit=True)\n","#     select_X_train = selection.transform(X_train)\n","    \n","#     # train model\n","#     selection_model = XGBClassifier(**xg_clf.best_params_)\n","#     selection_model.fit(select_X_train, y_train)\n","    \n","#     # eval model\n","#     select_X_test = selection.transform(X_test)\n","#     predictions = selection_model.predict(select_X_test)\n","#     precision = precision_score(y_test, predictions)\n","#     print(\"Thresh=%.3f, n=%d, Precision: %.2f%%\" % (thresh, select_X_train.shape[1], precision*100.0))\n","#     num_features = select_X_train.shape[1]\n","    \n","#     if precision > xg_final_params['precision'] and num_features >= 5: # must have at least 5 features\n","#         xg_final_params[\"threshold\"] = thresh\n","#         xg_final_params[\"precision\"] = precision\n","#         xg_final_params[\"num_features\"] = num_features\n","#         xg_final_params[\"columns\"] = X_train.columns[selection.get_support()].tolist()\n","\n","xg_final_params\n","\n","X_train_xg = X_train[xg_final_params[\"columns\"]]\n","X_test_xg = X_test[xg_final_params[\"columns\"]]\n","\n","xg_clf_filtered = XGBClassifier(**{'colsample_bytree': 1.0,\n","                                    'learning_rate': 0.05,\n","                                    'max_depth': 6,\n","                                    'min_child_weight': 3,\n","                                    'n_estimators': 10,\n","                                    'gamma':10})\n","xg_clf_filtered.fit(X_train_xg, y_train)\n","\n","print(f\"Test Accuracy Score: {metrics.accuracy_score(y_test, xg_clf_filtered.predict(X_test_xg))}\")\n","print(f\"Test Precision Score: {metrics.precision_score(y_test, xg_clf_filtered.predict(X_test_xg))}\")\n","\n","# X_train_xg = X_train[xg_final_params[\"columns\"]]\n","# X_test_xg = X_test[xg_final_params[\"columns\"]]\n","\n","# xg_clf_filtered = XGBClassifier(**xg_clf.best_params_)\n","# xg_clf_filtered.fit(X_train_xg, y_train)\n","\n","# print(f\"Test Accuracy Score: {metrics.accuracy_score(y_test, xg_clf_filtered.predict(X_test_xg))}\")\n","# print(f\"Test Precision Score: {metrics.precision_score(y_test, xg_clf_filtered.predict(X_test_xg))}\")\n","\n","The accuracy and precision has improved based on feature selection.\n","\n","feature_select_dict['xgb'] = xg_final_params[\"columns\"]\n","\n","import shap\n","\n","xg_clf_filtered = XGBClassifier(**{'colsample_bytree': 1.0,\n","                                    'learning_rate': 0.05,\n","                                    'max_depth': 6,\n","                                    'min_child_weight': 3,\n","                                    'n_estimators': 10,\n","                                    'gamma':10})\n","xg_clf_filtered.fit(X_train_xg, y_train)\n","explainer = shap.Explainer(xg_clf_filtered)\n","shap_values = explainer(X_train_xg)\n","\n","# visualise prediction explanation\n","shap.plots.waterfall(shap_values[0])\n","\n","# import shap\n","\n","# xg_clf_filtered = XGBClassifier(**xg_clf.best_params_)\n","# xg_clf_filtered.fit(X_train_xg, y_train)\n","# explainer = shap.Explainer(xg_clf_filtered)\n","# shap_values = explainer(X_train_xg)\n","\n","# # visualise prediction explanation\n","# shap.plots.waterfall(shap_values[0])\n","\n","# summarise the effects of all the features\n","\n","shap.plots.beeswarm(shap_values)\n","\n","# mean absolute value of the SHAP values\n","\n","shap.plots.bar(shap_values)\n","\n","## SVM\n","\n","%%time\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import svm\n","\n","\n","svm_clf = svm.SVC()\n","svm_clf.fit(X_train, y_train)\n","\n","# train\n","print(f\"Train Accuracy Score: {metrics.accuracy_score(y_train, svm_clf.predict(X_train))}\")\n","print(f\"Train Precision Score: {metrics.precision_score(y_train, svm_clf.predict(X_train))}\")\n","\n","# test\n","print(f\"Test Accuracy Score: {metrics.accuracy_score(y_test, svm_clf.predict(X_test))}\")\n","print(f\"Test Precision Score: {metrics.precision_score(y_test, svm_clf.predict(X_test))}\")\n","\n","# svm_params = {'C': [0.1, 1, 10, 100, 1000], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n","svm_params = {'C': [0.1, 1, 10, 100, 1000], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\n","\n","svm_clf = GridSearchCV(svm_clf, svm_params, scoring='precision', verbose=1, n_jobs=-1, cv=3)\n","svm_clf.fit(X_train, y_train)\n","\n","svm_clf.best_params_\n","\n","# train\n","print(f\"Train Accuracy Score: {metrics.accuracy_score(y_train, svm_clf.predict(X_train))}\")\n","print(f\"Train Precision Score: {metrics.precision_score(y_train, svm_clf.predict(X_train))}\")\n","\n","# test\n","print(f\"Test Accuracy Score: {metrics.accuracy_score(y_test, svm_clf.predict(X_test))}\")\n","print(f\"Test Precision Score: {metrics.precision_score(y_test, svm_clf.predict(X_test))}\")\n","\n","from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n","\n","# Build step forward feature selection\n","sfs1 = sfs(svm.SVC(**svm_clf.best_params_),\n","           k_features=8,\n","           forward=True,\n","           floating=False,\n","           verbose=2,\n","           scoring='precision',\n","           cv=5)\n","\n","# Perform SFFS\n","sfs1 = sfs1.fit(X_train, y_train)\n","\n","# features\n","feat_cols = list(sfs1.k_feature_idx_)\n","print(feat_cols)\n","svm_filtered_df = X_train.iloc[:,feat_cols]\n","\n","The optimal feature selection is based on 8 features to reach the maximum precision score of 0.618\n","\n","feature_select_dict['svm'] = svm_filtered_df.columns.tolist()\n","\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import svm\n","\n","svm_clf_tuned = svm.SVC(**svm_clf.best_params_)\n","X_train_svm = X_train[svm_filtered_df.columns.tolist()]\n","X_test_svm = X_test[svm_filtered_df.columns.tolist()]\n","svm_clf_tuned.fit(X_train_svm, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, svm_clf_tuned.predict(X_test_svm))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, svm_clf_tuned.predict(X_test_svm))}\")\n","\n","svm_clf.best_params_['probability'] = True\n","\n","# manual bypass - without feature selection\n","\n","feature_select_dict['svm'] = X_clf # all columns\n","svm_clf_tuned = svm.SVC(**svm_clf.best_params_)\n","svm_clf_tuned.fit(X_train, y_train)\n","\n","\n","\n","\n","\n","## SGD Classifier\n","Stochastic Gradient Descent (SGD): used for discriminative learning of linear classifiers under convex loss functions such as SVM and Logistic Regression.\n","\n","SGD = SGDClassifier(shuffle=False)\n","SGD.fit(X_train, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, SGD.predict(X_test))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, SGD.predict(X_test))}\")\n","\n","%%time\n","\n","from sklearn.model_selection import GridSearchCV\n","import tqdm\n","from tqdm import tqdm_notebook as tqdm\n","\n","\n","sgd_params = {\"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n","                \"penalty\": [\"l1\", \"l2\", \"none\"],}\n","\n","sgd_clf = GridSearchCV(SGD, sgd_params, scoring='precision', verbose=5, n_jobs=-1, cv=3)\n","sgd_clf.fit(X_train, y_train)\n","\n","sgd_clf.best_params_\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_train, sgd_clf.predict(X_train))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_train, sgd_clf.predict(X_train))}\")\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, sgd_clf.predict(X_test))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, sgd_clf.predict(X_test))}\")\n","\n","from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n","\n","# Build step forward feature selection\n","sfs1 = sfs(SGDClassifier(shuffle=False),\n","           k_features=9,\n","           forward=True,\n","           floating=False,\n","           verbose=2,\n","           scoring='precision',\n","           cv=5)\n","\n","# Perform SFFS\n","sfs1 = sfs1.fit(X_train, y_train)\n","\n","# features\n","feat_cols = list(sfs1.k_feature_idx_)\n","print(feat_cols)\n","X_train.iloc[:,feat_cols]\n","\n","The optimal number of features is 9, with a precision score of 0.747 respectively. Columns are cpi_growth, event_sentiment_score, returns, bol_wband_14, gdp_growth.\n","\n","X_train_sgd = X_train.iloc[:,feat_cols]\n","X_test_sgd = X_test.iloc[:,feat_cols]\n","\n","sgd_clf_tuned = SGDClassifier(**sgd_clf.best_params_, shuffle=False)\n","sgd_clf_tuned.fit(X_train_sgd, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, sgd_clf_tuned.predict(X_test_sgd))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, sgd_clf_tuned.predict(X_test_sgd))}\")\n","\n","feature_select_dict['sgd'] = X_train_sgd.columns.tolist()\n","\n","## Bernoulli NB\n","\n","bnb = BernoulliNB()\n","bnb.fit(X_train, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, bnb.predict(X_test))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, bnb.predict(X_test))}\")\n","\n","feature_select_dict['bnb'] = X_clf\n","\n","## Random Forest Classifier\n","Take quite long to run the gridsearchcv methodology. Reference link for hyperparameter: https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n","\n","from sklearn import metrics\n","\n","rf = RandomForestClassifier()\n","rf.fit(X_train, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, rf.predict(X_test))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, rf.predict(X_test))}\")\n","\n","Hyperparameters: <br>\n","1. n_estimators: number of trees in the forest of the model.\n","2. max_depth: maximum depth for each trees (each tree will expand until every lead is pure (means all same class)\n","3. min_samples_split: minimum number of samples required to split an internal leaf node. (ie number of leaf before you can split)\n","4. min_samples_leaf: minimum number of samples required tob at the leaf node.\n","5. n_estimators: number of trees, the more trees, the more time complexity.\n","\n","param_grid = {\n","    'bootstrap': [True, False],\n","    'max_depth': range(1, 5, 1),\n","    'max_features': ['auto', 'sqrt', 'log2'],\n","    'min_samples_leaf': range(1,10,2), # prevent overfitting, increase the leaf size\n","    'min_samples_split': range(1,10,2),\n","    'n_estimators': [50, 100, 200, 300, 400, 500],\n","    'criterion': ['gini', 'entropy']\n","}\n","\n","%%time\n","\n","rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, verbose=5, n_jobs=-1, scoring='precision')\n","rfc.fit(X_train, y_train)\n","\n","rfc.best_params_\n","\n","# the best params tuned for rfc\n","\n","\"\"\"\n","{'bootstrap': True,\n"," 'criterion': 'gini',\n"," 'max_depth': 2,\n"," 'max_features': 'log2',\n"," 'min_samples_leaf': 5,\n"," 'min_samples_split': 9,\n"," 'n_estimators': 200}\n","\"\"\"\n","\n","# train\n","print(f\"Train Accuracy Score: {metrics.accuracy_score(y_train, rfc.predict(X_train))}\")\n","print(f\"Train Precision Score: {metrics.precision_score(y_train, rfc.predict(X_train))}\")\n","\n","# test\n","print(f\"Test Accuracy Score: {metrics.accuracy_score(y_test, rfc.predict(X_test))}\")\n","print(f\"Test Precision Score: {metrics.precision_score(y_test, rfc.predict(X_test))}\")\n","\n","rf_clf_tuned = RandomForestClassifier(**rfc.best_params_)\n","rf_clf_tuned.fit(X_train, y_train)\n","\n","# import pickle\n","# import joblib\n","\n","# # save the output of gridsearch process to pickle file (dont need to keep tuning)\n","# filename = 'outputs/rfc_gridsearch_monthly.pickle'\n","# dbfile = open(filename, 'wb')\n","# pickle.dump(rfc, dbfile)\n","# dbfile.close()\n","\n","# # load the output of gridsearch trained process\n","# file = open('outputs/rfc_gridsearch_monthly.pickle', \"rb\")\n","# rfc = pickle.load(file)\n","\n","from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n","\n","%time\n","\n","rf = RandomForestClassifier(**rfc.best_params_)\n","# Build step forward feature selection\n","sfs1 = sfs(rf,\n","           k_features=9,\n","           forward=True,\n","           floating=False,\n","           verbose=2,\n","           scoring='precision',\n","           cv=5)\n","\n","# Perform SFFS\n","sfs1 = sfs1.fit(X_train, y_train)\n","\n","# features\n","feat_cols = list(sfs1.k_feature_idx_)\n","print(feat_cols)\n","X_train.iloc[:,feat_cols]\n","\n","X_train_rf = X_train.iloc[:,feat_cols]\n","X_test_rf = X_test.iloc[:,feat_cols]\n","\n","rf_clf_tuned = RandomForestClassifier(**rfc.best_params_)\n","rf_clf_tuned.fit(X_train_rf, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, rf_clf_tuned.predict(X_test_rf))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, rf_clf_tuned.predict(X_test_rf))}\")\n","\n","# feature_select_dict['rf'] = X_train_rf.columns.tolist()\n","\n","feature_select_dict['rf'] = X_train.columns.tolist()\n","\n","from sklearn.ensemble import RandomForestClassifier\n","\n","importances = rf_clf_tuned.feature_importances_\n","\n","final_df = pd.DataFrame({\"Features\": pd.DataFrame(X_train_rf).columns, \"Importances\": importances})\n","final_df.set_index('Importances')\n","\n","final_df = final_df.sort_values(\"Importances\")\n","\n","final_df.plot.bar(x='Features', y='Importances', color = 'teal')\n","\n","final_df\n","\n","### Selecting threshold based on precision score (additional testing)\n","\n","rfc\n","\n","from sklearn.feature_selection import SelectFromModel\n","from numpy import sort, array\n","from sklearn.metrics import precision_score\n","\n","# Fit model using each importance as a threshold\n","thresholds = sort(rf_clf_tuned.feature_importances_)\n","\n","rfc_final_params = {'threshold': 0, 'precision': 0, 'num_features': 0, 'columns': []}\n","\n","for thresh in thresholds:\n","    \n","    # select features using threshold\n","    selection = SelectFromModel(rf_clf_tuned, threshold=thresh, prefit=True)\n","    select_X_train = selection.transform(X_train_rf)\n","    \n","    # train model\n","    selection_model = RandomForestClassifier(**rfc.best_params_)\n","    selection_model.fit(select_X_train, y_train)\n","    \n","    # eval model\n","    select_X_test = selection.transform(X_test_rf)\n","    predictions = selection_model.predict(select_X_test)\n","    precision = precision_score(y_test, predictions)\n","    print(\"Thresh=%.3f, n=%d, Precision: %.2f%%\" % (thresh, select_X_train.shape[1], precision*100.0))\n","    num_features = select_X_train.shape[1]\n","    \n","    if precision > rfc_final_params['precision'] and num_features >= 5: # must have at least 5 features\n","        rfc_final_params[\"threshold\"] = thresh\n","        rfc_final_params[\"precision\"] = precision\n","        rfc_final_params[\"num_features\"] = num_features\n","        rfc_final_params[\"columns\"] = X_train_rf.columns[selection.get_support()].tolist()\n","\n","rfc_final_params\n","\n","X_train_rfc = X_train[rfc_final_params[\"columns\"]]\n","X_test_rfc = X_test[rfc_final_params[\"columns\"]]\n","\n","rfc_clf_filtered = RandomForestClassifier(**rfc.best_params_)\n","rfc_clf_filtered.fit(X_train_rfc, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, rfc_clf_filtered.predict(X_test_rfc))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, rfc_clf_filtered.predict(X_test_rfc))}\")\n","\n","feature_select_dict\n","\n","# hardcode normal random forest\n","\n","X_train_rf = X_train.iloc[:,feat_cols]\n","X_test_rf = X_test.iloc[:,feat_cols]\n","\n","rf_clf_tuned = RandomForestClassifier()\n","rf_clf_tuned.fit(X_train_rf, y_train)\n","\n","print(f\"Accuracy Score: {metrics.accuracy_score(y_test, rf_clf_tuned.predict(X_test_rf))}\")\n","print(f\"Precision Score: {metrics.precision_score(y_test, rf_clf_tuned.predict(X_test_rf))}\")\n","\n","feature_select_dict['rf'] = X_train_rf.columns.tolist()\n","\n","## Model Performance\n","\n","print(\"Model performance with returns prediction\")\n","model_performance([xg_clf_filtered, svm_clf_tuned, sgd_clf_tuned, bnb, rf_clf_tuned], [\"xgb\", 'svm', 'sgd', 'bnb', 'rf'], feature_select_dict)\n","\n","# Ensemble Models\n","\n","base_models = [\n","    ('xgb', xg_clf_filtered),\n","    ('svm', svm_clf_tuned),\n","    ('sgd', sgd_clf_tuned),\n","    ('bnb', bnb),\n","    ('rf', rf_clf_tuned)\n","]\n","\n","meta_model = svm.SVC(C=100, gamma=1, probability=True)\n","\n","%%time\n","from sklearn.ensemble import StackingClassifier\n","\n","stacked_model = StackingClassifier(estimators=base_models, final_estimator=meta_model, n_jobs=-1, verbose=5)\n","stacked_model.fit(X_train, y_train)\n","stacked_model.score(X_test, y_test)\n","\n","%%time\n","\n","stacked_model_svm = StackingClassifier(estimators=base_models, final_estimator=xg_clf_filtered, n_jobs=-1, verbose=5)\n","stacked_model_svm.fit(X_train, y_train)\n","stacked_model_svm.score(X_test, y_test)\n","\n","## for plotting\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","classes = np.unique(y_test)\n","fig, ax = plt.subplots()\n","cm = metrics.confusion_matrix(y_test, stacked_model_svm.predict(X_test), labels=classes)\n","sns.heatmap(cm, annot=True, fmt='d', cmap=plt.cm.Blues, cbar=False)\n","ax.set(xlabel=\"Pred\", ylabel=\"True\", title=\"Confusion matrix\")\n","ax.set_yticklabels(labels=classes, rotation=0)\n","plt.show()\n","\n","# roc curve and auc\n","from sklearn.metrics import roc_auc_score, roc_curve\n","\n","def plot_roc_curve(fpr, tpr):\n","    plt.plot(fpr, tpr, color='orange', label='ROC')\n","    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend()\n","    plt.show()\n","    \n","probs = stacked_model_svm.predict_proba(X_test)\n","probs = probs[:, 1]\n","auc = roc_auc_score(y_test, probs)\n","print('AUC: %.2f' % auc)\n","fpr, tpr, thresholds = roc_curve(y_test, probs)\n","plot_roc_curve(fpr, tpr)\n","\n","feature_select_dict['stack'] = X_clf\n","\n","print(\"Model performance with returns prediction\")\n","model_performance([xg_clf_filtered, svm_clf_tuned, sgd_clf_tuned, bnb, rf_clf_tuned, stacked_model], [\"xgb\", 'svm', 'sgd', 'bnb', 'rf', 'stack'], feature_select_dict)\n","\n","## Export Model\n","\n","# # FOR XGBOOST\n","# # save model to pickle\n","# import pickle\n","# import joblib\n","\n","# # save the model to disk\n","# filename = 'outputs/xgb_monthly_combined_ess_features.pickle'\n","# dbfile =  open(filename, 'wb')\n","# pickle.dump(xg_clf_filtered, dbfile)\n","# dbfile.close()\n","\n","# scaler_filename = \"outputs/min_max_scaler_monthly_combined_ess_features.save\"\n","# joblib.dump(min_max_scaler, scaler_filename)\n","\n","# filename = \"outputs/feature_select_dict_monthly_ess_features.pickle\" # be aware and remember to change accordingly\n","# dbfile =  open(filename, 'wb')\n","# pickle.dump(feature_select_dict['xgb'], dbfile)\n","# dbfile.close()\n","\n","# # FOR RANDOM FOREST\n","# # save model to pickle\n","# import pickle\n","# import joblib\n","\n","# # save the model to disk\n","# filename = 'outputs/rf_monthly_combined_ess_features.pickle'\n","# dbfile =  open(filename, 'wb')\n","# pickle.dump(rf_clf_tuned, dbfile)\n","# dbfile.close()\n","\n","# scaler_filename = \"outputs/min_max_scaler_monthly_combined_ess_features.save\"\n","# joblib.dump(min_max_scaler, scaler_filename)\n","\n","# filename = \"outputs/rf_feature_select_dict_monthly_ess_features.pickle\" # be aware and remember to change accordingly\n","# dbfile =  open(filename, 'wb')\n","# pickle.dump(feature_select_dict['rf'], dbfile)\n","# dbfile.close()"]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"import_library.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPmw8SORFue/AUQoTBCBlKw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"KlO40dhl8sdN"},"outputs":[],"source":["## Useful Functions for Repeated Use\n","This notebook serves to input repeated functions for ease of script coding.\n","\n","# useful import libraries\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import joblib\n","import string\n","import re\n","\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, roc_auc_score, f1_score\n","from pandas.tseries.offsets import MonthEnd\n","\n","# models and inputs files\n","with_ess_features = True # condition if we want to use ESS features or not\n","\n","# MACRO\n","file = open('outputs/US_data.pickle', \"rb\")\n","us_macro_df = pickle.load(file)\n","\n","file = open('outputs/CN_data.pickle', \"rb\")\n","cn_macro_df = pickle.load(file)\n","\n","# MODELS FOR SENTIMENT\n","file = open('outputs/tfidf_vect_tuned.pickle', \"rb\")\n","tfidf_vect = pickle.load(file)\n","\n","if with_ess_features==True:\n","    \n","    file = open('outputs/xgb_monthly_combined_ESS_features.pickle', \"rb\") # under 04D Classification Model - latest xgb 220310\n","    xgb_monthly_combined = pickle.load(file)\n","\n","    # ESS features (comment it out if using)\n","    filename = 'outputs/min_max_scaler_monthly_combined_ESS_features.save'\n","    min_max_scaler_monthly = joblib.load(filename)\n","\n","    file = open('outputs/feature_select_dict_monthly_ESS_features.pickle', \"rb\")\n","    feature_select_dict_monthly = pickle.load(file)\n","\n","### Based on Multicollinearity\n","Filtered based on manual and analysis on 04D script for classification model\n","\n","# features to remove (manual basis) - for technical indicators, we will be using mid_window # 'percentile_rank_ratings_event_sentiment_score', 'percentile_rank_product_event_sentiment_score',\n","\n","remove_features = ['model_score', 'rsi_5', 'rsi_50', 'evm_5', 'evm_50', 'bol_buy_5', 'bol_buy_50', 'bol_wband_5', 'bol_wband_50', 'adx_5', 'adx_50'] + ['percentile_rank_mergers_event_sentiment_score', 'percentile_rank_mergers_model_score', 'mergers_model_score', 'percentile_rank_product_model_score', 'product_model_score', 'percentile_rank_earnings_event_sentiment_score', 'percentile_rank_earnings_model_score', 'earnings_model_score',  'percentile_rank_ratings_model_score', 'ratings_model_score', 'percentile_rank_regulatory_event_sentiment_score', 'percentile_rank_regulatory_model_score', 'regulatory_model_score'] + ['percentile_rank_ratings_event_sentiment_score', 'percentile_rank_product_event_sentiment_score']\n","\n","# training variables\n","\n","X_clf = [\"model_score\", \"event_sentiment_score\", \"vol\",  \"news_spikes_m\", \"returns\", \"gdp_growth_us\", 'cpi_growth_us', 'unemployment_rate_us', \"gdp_growth_cn\", 'cpi_growth_cn', 'rsi_5', 'rsi_14', 'rsi_50', 'evm_5', 'evm_14', 'evm_50', 'bol_buy_5', 'bol_buy_14', 'bol_buy_50', 'bol_wband_5', 'bol_wband_14', 'bol_wband_50', 'adx_5', 'adx_14', 'adx_50', 'macd_12_26']  + ['mergers_event_sentiment_score', 'product_event_sentiment_score', 'earnings_event_sentiment_score', 'ratings_event_sentiment_score', 'regulatory_event_sentiment_score']  # X variables for classification\n","X_reg = [\"model_score\", \"event_sentiment_score\", \"vol\",  \"news_spikes_m\", \"returns\", \"gdp_growth_us\", 'cpi_growth_us', 'unemployment_rate_us', \"gdp_growth_cn\", 'cpi_growth_cn', 'rsi_5', 'rsi_14', 'rsi_50', 'evm_5', 'evm_14', 'evm_50', 'bol_buy_5', 'bol_buy_14', 'bol_buy_50', 'bol_wband_5', 'bol_wband_14', 'bol_wband_50', 'adx_5', 'adx_14', 'adx_50', 'macd_12_26']  + ['mergers_event_sentiment_score', 'product_event_sentiment_score', 'earnings_event_sentiment_score', 'ratings_event_sentiment_score', 'regulatory_event_sentiment_score']  # X variables for regression model\n","\n","# after removing the features\n","X_clf = [i for i in X_clf if i not in remove_features]\n","X_reg = [i for i in X_reg if i not in remove_features]\n","\n","y_clf = \"returns_movement\"    \n","y_reg = \"returns_lead_1\"\n","\n","# if not using ESS features\n","\n","if with_ess_features==False:\n","    additional_removal = ['mergers_event_sentiment_score',\n","    'product_event_sentiment_score',\n","    'earnings_event_sentiment_score',\n","    'ratings_event_sentiment_score',\n","    'regulatory_event_sentiment_score']\n","\n","    X_clf = [i for i in X_clf if i not in additional_removal]\n","\n","# script 03 general function\n","\n","def aggregate_period(df, period, resample='mean'): # checked\n","    \"\"\"\n","    Aggregating data based on required fields\n","    Period: D, W, M\n","    \"\"\"\n","    if resample == 'mean':\n","        if period == 'D':\n","            df = df.resample('D').mean()\n","\n","        elif period == 'W':\n","            df = df.resample('W-FRI').mean()\n","\n","        elif period == 'M':\n","            df = df.resample('M').mean()\n","\n","        else:\n","            raise Exception('Invalid timeframe from input')\n","\n","    elif resample == 'last':\n","        if period == 'D':\n","            df = df.resample('D').last()\n","\n","        elif period == 'W':\n","            df = df.resample('W-FRI').last()\n","\n","        elif period == 'M':\n","            df = df.resample('M').last()\n","\n","        else:\n","            raise Exception('Invalid timeframe from input')\n","            \n","    elif resample == 'sum':\n","        if period == 'D':\n","            df = df.resample('D').sum()\n","\n","        elif period == 'W':\n","            df = df.resample('W-FRI').sum()\n","\n","        elif period == 'M':\n","            df = df.resample('M').sum()\n","\n","        else:\n","            raise Exception('Invalid timeframe from input')\n","            \n","    # ffill then bfill\n","    df = df.ffill(axis=0).bfill(axis=0)\n","    \n","    return df\n","\n","def standardize_col_names(df, remove_punct=True): # checked\n","    \"\"\" \n","    Converts all column names to lower case replacing\n","    whitespace of any length with a single underscore.\n","    Also, remove punctuations if included.\n","\n","    \"\"\"\n","\n","    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n","\n","    for c in df.columns:\n","\n","        c_mod = c.lower()\n","\n","        if remove_punct:            \n","            c_mod = c_mod.translate(translator)\n","\n","        c_mod = '_'.join(c_mod.split(' '))\n","\n","        if c_mod[-1] == '_':\n","            c_mod = c_mod[:-1]\n","\n","        c_mod = re.sub(r'\\_+', '_', c_mod)\n","\n","        df.rename({c: c_mod}, inplace=True, axis=1)\n","\n","    return df\n","\n","Note: Currently using event_sentiment_score to predict sentiment movement, similar to our model\n","\n","def returns_movement(s):\n","    \"\"\"\n","    Indicates the actual returns from historical data one period ahead\n","    \"\"\"\n","    \n","    if (s['returns_lead_1'] > 0):\n","        return 1\n","    \n","    else:\n","        return 0\n","    \n","\n","def financial_returns_movement(s):\n","    \"\"\"\n","    Indicates the actual returns from historical data one period ahead\n","    \"\"\"\n","    \n","    if (s['msci_financial_returns_lead_1'] > 0):\n","        return 1\n","    \n","    else:\n","        return 0\n","    \n","    \n","def msci_singapore_returns_movement(s):\n","    \"\"\"\n","    Indicates the actual returns from historical data one period ahead\n","    \"\"\"\n","    \n","    if (s['msci_singapore_returns_lead_1'] > 0):\n","        return 1\n","    \n","    else:\n","        return 0\n","    \n","def msci_china_returns_movement(s):\n","    \"\"\"\n","    Indicates the actual returns from historical data one period ahead\n","    \"\"\"\n","    \n","    if (s['msci_china_returns_lead_1'] > 0):\n","        return 1\n","    \n","    else:\n","        return 0\n","\n","def performance(result, bank, period, ow_uw=False): # checked\n","    \"\"\"\n","    Performance metrics based on the periodic time series\n","    \"\"\"\n","\n","    # filter the relevant banks first\n","    result = result[(result['sedol'] == bank)]\n","    result.sort_index(inplace=True)\n","\n","    # cumulative returns - for both bm and pm\n","    return_bm_cum = result.iloc[-1]['return_bm_cum']-1\n","    return_mp_cum = result.iloc[-1]['return_mp_cum']-1\n","\n","    if period == \"M\":\n","        n_months= len(result.index)\n","        n_years=n_months/12\n","        \n","    elif period == \"W\":\n","        n_weeks= len(result.index)\n","        n_years=n_weeks/52\n","    \n","    else:\n","        raise Exception('Period window is not compatible. Please check if the time series is either weekly or monthly.')\n","    \n","    print(f\"{bank}: {n_years} years of datasets\")\n","    # auc=roc_auc_score(result['returns_movement'], result['position'])\n","    f1score=f1_score(result['returns_movement'], result['position'], average='weighted')\n","    precision=precision_score(result['returns_movement'], result['position'], average='weighted')\n","\n","    ## trade lot and positive\n","    if ow_uw: # overweight underweight winrate calculation based on excess return\n","        winrate=(np.array(result[(result['position_lag_1']!=0)]['excess_return_mp'])>0).mean()\n","            \n","    else:\n","        winrate=(np.array(result[(result['position_lag_1']!=0)]['return_mp'])>1).mean()\n","\n","    anl_rtn_bm =(result[['return_bm','return_mp']].cumprod()**(1/n_years)).iloc[-1,0]\n","    anl_rtn_mp=(result[['return_bm','return_mp']].cumprod()**(1/n_years)).iloc[-1,1]\n","    anl_ex_rtn= anl_rtn_mp-anl_rtn_bm\n","    \n","    turnover=abs(result['position'].diff()).sum()/n_years/2\n","    \n","    ## sharpe ratio\n","    result['year']=pd.to_datetime(result.index).year\n","    result_year=result.groupby('year').agg('last')\n","    return_bm_y=(result_year['return_bm_cum']/result_year['return_bm_cum'].shift(1)-1).dropna() # series, not single value\n","    return_mp_y=(result_year['return_mp_cum']/result_year['return_mp_cum'].shift(1)-1).dropna()# series, not single value\n","    return_excess_y=return_mp_y-return_bm_y # portfolio - benchmark\n","    volatility_bm=return_bm_y.std() # to display as output performance\n","    volatility_mp=return_mp_y.std() # to display as output performance\n","    avg_ex_rtn=return_excess_y.mean()\n","\n","    if return_excess_y.std() == 0:\n","        sharpe_ratio = \"nil\"\n","    else:\n","        sharpe_ratio=avg_ex_rtn/return_excess_y.std()\n","            \n","    ## max_drawdown: buy and hold\n","    max_drawdown_bm=0\n","    for i in range(result.shape[0]):\n","        temp=result['return_bm_cum'][i:].min()/result['return_bm_cum'][i]-1\n","        \n","        if temp<max_drawdown_bm:\n","            max_drawdown_bm=temp\n","    \n","    ## max_drawdown: sentiment\n","    max_drawdown_mp=0\n","    for i in range(result.shape[0]):\n","        temp=result['return_mp_cum'][i:].min()/result['return_mp_cum'][i]-1\n","        \n","        if temp<max_drawdown_mp:\n","            max_drawdown_mp=temp\n","\n","    return [anl_rtn_bm,anl_rtn_mp,anl_ex_rtn,winrate,turnover,volatility_bm,volatility_mp,max_drawdown_bm,max_drawdown_mp,return_bm_cum,return_mp_cum,sharpe_ratio,f1score,precision]\n","\n","def performance_breakdown(result, bank, period, ow_uw=False): # checked\n","    \"\"\"\n","    Detailed breakdown of performance metrics based on yearly basis\n","    \"\"\"\n","    \n","    ldf = []\n","    \n","    for y in range(2019, 2023):\n","        \n","        # filtering the specific date\n","        train_df = result[(result.index >= pd.to_datetime(f'{y}-01-01')) & (result.index <= pd.to_datetime(f'{y}-12-31'))]\n","        train_df.reset_index(inplace=True, drop=True)\n","        train_df.sort_index(inplace=True) # sort based on date\n","        \n","        # cumulative returns - for both bm and pm\n","        return_bm_cum = (train_df['return_bm']).cumprod().iloc[-1] -1\n","        return_mp_cum = (train_df['return_mp']).cumprod().iloc[-1] -1\n","        \n","        \n","        if period == \"M\":\n","            n_months= len(train_df.index)\n","            n_years=n_months/12\n","\n","        elif period == \"W\":\n","            n_weeks= len(train_df.index)\n","            n_years=n_weeks/52\n","            \n","        else:\n","            raise Exception('Period window is not compatible. Please check if the time series is either weekly or monthly.')\n","\n","        # auc=roc_auc_score(train_df['returns_movement'], train_df['position'])\n","        f1score=f1_score(train_df['returns_movement'], train_df['position'], average='weighted')\n","        precision=precision_score(train_df['returns_movement'], train_df['position'], average='weighted')\n","\n","        if ow_uw: # overweight underweight winrate calculation based on excess return\n","            winrate=(np.array(train_df[(train_df['position_lag_1']!=0)]['excess_return_mp'])>0).mean()\n","            \n","        else:\n","            winrate=(np.array(train_df[(train_df['position_lag_1']!=0)]['return_mp'])>1).mean()\n","\n","        anl_rtn_bm=(train_df[['return_bm','return_mp']].cumprod()**(1/n_years)).iloc[-1,0]\n","        anl_rtn_mp=(train_df[['return_bm','return_mp']].cumprod()**(1/n_years)).iloc[-1,1]\n","        anl_ex_rtn= anl_rtn_mp-anl_rtn_bm\n","\n","        turnover=abs(train_df['position'].diff()).sum()/n_years/2\n","\n","        ## sharpe ratio\n","        train_df['year']=pd.to_datetime(train_df.index).year\n","        result_year=train_df # no need to agg as df is in yearly freq\n","        return_bm_y=(result_year['return_bm_cum']/result_year['return_bm_cum'].shift(1)-1).dropna() # series, not single value\n","        return_mp_y=(result_year['return_mp_cum']/result_year['return_mp_cum'].shift(1)-1).dropna()# series, not single value\n","        return_excess_y=return_mp_y-return_bm_y # portfolio - benchmark\n","        volatility_bm=return_bm_y.std()\n","        volatility_mp=return_mp_y.std()\n","        avg_ex_rtn=return_excess_y.mean()\n","        \n","        if return_excess_y.std() == 0: # for extreme values checking\n","            sharpe_ratio=\"nil\"\n","        else:\n","            sharpe_ratio=avg_ex_rtn/return_excess_y.std()\n","\n","        ## max_drawdown: buy and hold\n","        max_drawdown_bm=0\n","        \n","        for i in range(train_df.shape[0]):\n","            temp=train_df['return_bm_cum'][i:].min()/train_df['return_bm_cum'][i]-1\n","\n","            if temp<max_drawdown_bm:\n","                max_drawdown_bm=temp\n","\n","        ## max_drawdown: sentiment\n","        max_drawdown_mp=0\n","        for i in range(train_df.shape[0]):\n","            temp=train_df['return_mp_cum'][i:].min()/train_df['return_mp_cum'][i]-1\n","\n","            if temp<max_drawdown_mp:\n","                max_drawdown_mp=temp\n","        \n","        df1 = pd.DataFrame(index=['Annualised Return (Buy Hold)','Annualised Return (Sentiment)','Annualised Excess Return', 'Winrate', \"Annualised Turnover\", 'Volatility (Buy Hold)', 'Volatility (Sentiment)', 'Max Drawdown (Buy Hold)', 'Max Drawdown (Sentiment)',\"Cumulative Return (Buy Hold)\", \"Cumulative Return (Sentiment)\", 'Sharpe Ratio', 'F1 Score', \"Precision\"])\n","        df1[f\"{y}\"] = [anl_rtn_bm,anl_rtn_mp,anl_ex_rtn,winrate,turnover,volatility_bm,volatility_mp,max_drawdown_bm,max_drawdown_mp,return_bm_cum,return_mp_cum,sharpe_ratio,f1score,precision]\n","\n","        ldf.append(df1)\n","    \n","    \n","    df = pd.concat(ldf, axis=1)\n","    \n","    return df\n","\n","def perf_format(performance_df, benchmark=False): # checked\n","    \"\"\"\n","    Formatting the performance dataframe into desired precision and format\n","    \"\"\"\n","    if benchmark==True:\n","        # converting certain rows into percentage\n","        performance.index_name = 'Metrics'\n","        performance_df = performance_df.round(decimals=5)\n","        performance_df.loc['Annualised Return (Benchmark)'] = ((performance_df.loc['Annualised Return (Benchmark)']-1).apply('{:.02%}'.format))\n","        performance_df.loc['Annualised Return (Sentiment)'] = ((performance_df.loc['Annualised Return (Sentiment)']-1).apply('{:.02%}'.format))\n","        performance_df.loc['Annualised Excess Return'] = ((performance_df.loc['Annualised Excess Return']).apply('{:.02%}'.format))\n","        performance_df.loc['Winrate'] = ((performance_df.loc['Winrate']).apply('{:.02%}'.format))\n","        performance_df.loc['Volatility (Benchmark)'] = ((performance_df.loc['Volatility (Benchmark)']).apply('{:.02%}'.format))\n","        performance_df.loc['Volatility (Sentiment)'] = ((performance_df.loc['Volatility (Sentiment)']).apply('{:.02%}'.format))\n","        performance_df.loc['Max Drawdown (Benchmark)'] = ((performance_df.loc['Max Drawdown (Benchmark)']).apply('{:.02%}'.format))\n","        performance_df.loc['Max Drawdown (Sentiment)'] = ((performance_df.loc['Max Drawdown (Sentiment)']).apply('{:.02%}'.format))\n","\n","        \n","    else:\n","        # converting certain rows into percentage\n","        performance.index_name = 'Metrics'\n","        performance_df = performance_df.round(decimals=5)\n","        performance_df.loc['Annualised Return (Buy Hold)'] = ((performance_df.loc['Annualised Return (Buy Hold)']-1).apply('{:.02%}'.format))\n","        performance_df.loc['Annualised Return (Sentiment)'] = ((performance_df.loc['Annualised Return (Sentiment)']-1).apply('{:.02%}'.format))\n","        performance_df.loc['Annualised Excess Return'] = ((performance_df.loc['Annualised Excess Return']).apply('{:.02%}'.format))\n","        performance_df.loc['Winrate'] = ((performance_df.loc['Winrate']).apply('{:.02%}'.format))\n","        performance_df.loc['Volatility (Buy Hold)'] = ((performance_df.loc['Volatility (Buy Hold)']).apply('{:.02%}'.format))\n","        performance_df.loc['Volatility (Sentiment)'] = ((performance_df.loc['Volatility (Sentiment)']).apply('{:.02%}'.format))\n","        performance_df.loc['Max Drawdown (Buy Hold)'] = ((performance_df.loc['Max Drawdown (Buy Hold)']).apply('{:.02%}'.format))\n","        performance_df.loc['Max Drawdown (Sentiment)'] = ((performance_df.loc['Max Drawdown (Sentiment)']).apply('{:.02%}'.format))\n","\n","    return performance_df\n","\n","# for classification\n","def _train_test_split(df, entity_list, test_date): # checked\n","    \"\"\"\n","    Allow iterative train test split for the dataframe\n","    \"\"\"\n","    \n","    df.sort_index(axis=0, ascending=True, inplace=True)\n","\n","    # train test split based on date\n","    X = df[X_clf]\n","    X_train = X[X.index < test_date]\n","    X_test = X[X.index >= test_date]\n","\n","    y = df[f'{y_clf}']\n","    y_train = y[y.index < test_date]\n","    y_test = y[y.index >= test_date]\n","\n","    return X_train, y_train, X_test, y_test\n","\n","def model_performance(models, model_names, feature_select_dict): # checked\n","    \"\"\"\n","    Machine learning related performance metrics, specifically for classification models\n","    \"\"\"\n","    df = pd.DataFrame(columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 score', 'ROC AUC'])\n","\n","    for n, model in enumerate(models):\n","        name = model_names[n]\n","        y_pred = model.predict(X_test[feature_select_dict[name]])\n","\n","        acc = metrics.accuracy_score(y_test, y_pred)\n","        prec = metrics.precision_score(y_test, y_pred)\n","        recall = metrics.recall_score(y_test, y_pred)\n","        f1 = metrics.f1_score(y_test, y_pred)\n","        roc_auc = metrics.roc_auc_score(y_test, y_pred)\n","\n","        #append row to df\n","        df = df.append(\n","            {\n","                'Model' : name,\n","                'Accuracy': acc,\n","                'Precision': prec,\n","                'Recall': recall,\n","                'F1 score': f1,\n","                'ROC AUC': roc_auc\n","            }, ignore_index = True)\n","\n","    return df.set_index('Model').transpose()\n","\n","def prediction_processing(df, model, sentiment, freq, quantile=0.7, ess_threshold=0.2, returns_threshold=0.5, news_spikes_threshold=2.0): # checked\n","    \"\"\"\n","    Data manipulation for model prediction, default process to run the model\n","    \"\"\"\n","    \n","    # handling of frequency prediction (ie, weekly or monthly)\n","    if freq == \"W\":\n","        min_max_scaler = min_max_scaler_weekly\n","        feature_select_dict = feature_select_dict_weekly\n","    \n","    elif freq == \"M\":\n","        min_max_scaler = min_max_scaler_monthly\n","        feature_select_dict = feature_select_dict_monthly\n","    \n","    # scaling and transformation \n","    X = df[X_clf]\n","    X[X_clf] = min_max_scaler.transform(X[X_clf])\n","\n","    # Model Prediction\n","    result = model.predict(X[feature_select_dict])\n","    pred_result = model.predict_proba(X[feature_select_dict])\n","    df['returns_predict'] = result\n","    df['returns_predict_proba'] = pred_result[:,1] # get the prob of 1\n","    \n","    if sentiment:\n","    \n","        df['position'] = df.apply(lambda r: model_positions(r, True, True, quantile, ess_threshold, returns_threshold, news_spikes_threshold), axis=1)\n","        \n","    else: # use model as prediction\n","        \n","        df['position'] = df.apply(lambda r: model_positions(r, False, True, quantile, ess_threshold, returns_threshold, news_spikes_threshold), axis=1)\n","        \n","    df['position_lag_1'] = df['position'].shift(1) # use previous months position to calculate performance metrics\n","\n","    # Get the weekly log returns (assuming i make a decision on a daily basis)\n","    df['return_bm'] = df['returns']+1 # shift by 1 period (ie, curr - prev row)\n","    df['return_bm_cum'] = (df['return_bm']).cumprod()\n","\n","    df['return_mp'] = df.apply(lambda x: x['return_bm'] if x['position_lag_1'] == 1 else 1.0, axis=1) # finding the previous position and multiply it by current month returns\n","    df['return_mp_cum'] = (df['return_mp']).cumprod()\n","    \n","    # excess return mp for winrate calculation\n","    df['excess_return_mp'] = df['return_mp'] - df['return_bm']\n","    \n","    # creating strategy directions: both movement is for one period forward\n","    df['sentiment_movement'] = df['event_sentiment_score'].apply(lambda x: 1 if x > 0 else 0)\n","    df['returns_movement'] = df.apply(lambda x: returns_movement(x), axis=1)\n","    \n","    return df\n","\n","def prediction_processing_financials(df, hk_financials_sedol_list, use_predict_proba, quantile, returns_threshold, news_spikes_threshold):\n","    \"\"\"\n","    Model positioning for msci financials\n","    \"\"\"\n","    df = aggregate_financials(df, hk_financials_sedol_list, use_predict_proba)\n","        \n","    df['position'] = df.apply(lambda r: financial_positions(r, True, quantile, returns_threshold, news_spikes_threshold), axis=1)        \n","    df['position_lag_1'] = df['position'].shift(1) # use previous months position to calculate performance metrics\n","    \n","    # Get the monthly log returns (assuming i make a decision on a daily basis)\n","    df['return_bm'] = df['msci_financial_returns']+1 # shift by 1 period (ie, curr - prev row)\n","    df['return_bm_cum'] = (df['return_bm']).cumprod()\n","    \n","    df['return_mp'] = df.apply(lambda x: x['return_bm'] if x['position_lag_1'] == 1 else 1.0, axis=1) # finding the previous position and multiply it by current month returns\n","    df['return_mp_cum'] = (df['return_mp']).cumprod()\n","    \n","    # excess return mp for winrate calculation\n","    df['excess_return_mp'] = df['return_mp'] - df['return_bm']\n","    \n","    # creating strategy directions: movement is one period forward\n","    df['returns_movement'] = df.apply(lambda x: financial_returns_movement(x), axis=1)\n","\n","    return df\n","\n","def aggregate_financials(df, hk_financials_sedol_list, use_predict_proba=True):\n","    \"\"\"\n","    Aggregate the variables based on the weighted average of the individual financial stocks composition, using predict_proba\n","    \"\"\"\n","    # weighted returns based on AI/ML, using predict_proba\n","    df['tot_fin_weights'] = 1 - df['others'] # total overall financial weights\n","    df['returns_predict'] = calculate_returns_predict(df, hk_financials_sedol_list, use_predict_proba)\n","    \n","    # percentile_rank_ratings_event_sentiment_score, percentile_rank_product_event_sentiment_score, percentile_rank_regulatory_event_sentiment_score, news_spikes_m\n","    df['regulatory_event_sentiment_score'] = calculate_ess_group(df, hk_financials_sedol_list, \"regulatory_event_sentiment_score\")\n","    df['product_event_sentiment_score'] = calculate_ess_group(df, hk_financials_sedol_list, \"product_event_sentiment_score\")\n","    df['ratings_event_sentiment_score'] = calculate_ess_group(df, hk_financials_sedol_list, \"ratings_event_sentiment_score\")\n","    df['news_spikes_m'] = calculate_ess_group(df, hk_financials_sedol_list, \"news_spikes_m\")\n","    df['macd_12_26'] = calculate_ess_group(df, hk_financials_sedol_list, \"macd_12_26\")\n","\n","    # percentile ranking allocation    \n","    \n","    event_grouping_list = ['regulatory', 'product', 'ratings', 'macd_12_26']\n","    \n","    # filter out the important group list\n","    for e in event_grouping_list:\n","        if e == \"macd_12_26\":\n","            df[f\"percentile_rank_{e}\"] = df[f'{e}'].rank(pct=True)\n","            continue\n","        # percentile rank for the col\n","        df[f\"percentile_rank_{e}_event_sentiment_score\"] = df[f'{e}_event_sentiment_score'].rank(pct=True)\n","    \n","    required_cols = [ # relevant columns for the remaining codes\n","        'returns_predict',\n","        'msci_china_price',\n","        'msci_china_returns',\n","        'msci_china_returns_lead_1',\n","        'msci_financial_price',\n","        'msci_financial_returns', \n","        'msci_financial_returns_lead_1',\n","        'tot_fin_weights',\n","        'others',\n","        'percentile_rank_regulatory_event_sentiment_score', \n","        'percentile_rank_product_event_sentiment_score', \n","        'percentile_rank_ratings_event_sentiment_score',\n","        'percentile_rank_macd_12_26',\n","        'news_spikes_m'\n","    ]\n","    \n","    df = df[required_cols]\n","    \n","    return df\n","\n","def calculate_returns_predict(df, hk_financials_sedol_list, use_predict_proba):\n","    \"\"\"\n","    To calculate the returns predict proba based on aggregated weights and individual firm predict proba\n","    \"\"\"\n","    ldf = []\n","    if use_predict_proba:\n","        \n","        for sedol in hk_financials_sedol_list:\n","            df[f'{sedol}_weighted_returns_predict_proba'] = df[f'{sedol}_returns_predict_proba']*df[sedol]/df['total_hk_fin_weights']\n","            ldf.append(df[[f'{sedol}_weighted_returns_predict_proba']])\n","    \n","    else:\n","        \n","        for sedol in hk_financials_sedol_list:\n","            df[f'{sedol}_weighted_returns_predict'] = df[f'{sedol}_returns_predict']*df[sedol]/df['total_hk_fin_weights']\n","            ldf.append(df[[f'{sedol}_weighted_returns_predict']])\n","    \n","    new_df = pd.concat(ldf, axis=1)\n","\n","    return new_df.sum(axis=1).to_list()\n","\n","def calculate_ess_group(df, hk_financials_sedol_list, col_name):\n","    \"\"\"\n","    To calculate the returns predict proba based on aggregated weights and individual firm predict proba\n","    \"\"\"\n","    \n","    ldf = []\n","    for sedol in hk_financials_sedol_list:\n","        df[f'{sedol}_weighted_{col_name}'] = df[f'{sedol}_{col_name}']*df[sedol]/df['total_hk_fin_weights']\n","        ldf.append(df[[f'{sedol}_weighted_{col_name}']])\n","    \n","    new_df = pd.concat(ldf, axis=1)\n","    \n","    return new_df.sum(axis=1).to_list()\n","\n","# script 05 general functions\n","\n","def model_positions(row, sentiment=False, ess_x_spikes=False, quantile=0.7, ess_threshold=0.2, returns_threshold=0.5, news_spikes_threshold=2.0): # checked\n","    \"\"\"\n","    Returns position as 1, 0 for Buy and Hold respectively \n","    based on the predicted price value and threshold\n","    FYI: predicting forward returns\n","    \"\"\"\n","\n","    if sentiment: # using current ess to make positioning for future returns outcome\n","        \n","        if row['event_sentiment_score'] > ess_threshold: # based on optimal threshold calculated based on 05D monthly datasets\n","            output = 1\n","        else:\n","            output = 0\n","    \n","    else: # following the score based on predicted returns\n","    \n","        if row['returns_predict_proba'] > returns_threshold: # same as using returns_predict > 0\n","            output = 1\n","\n","        else:\n","            if row['percentile_rank_regulatory_event_sentiment_score'] > quantile: # rule based regime override\n","                output = 1\n","            else:\n","                output = 0\n","        \n","        # ultimate check on buy strategy:\n","        if row['percentile_rank_product_event_sentiment_score'] > quantile: # most positive sentiment score \n","\n","            output = 0 # due to the strong negative correlation for financials\n","        \n","        if ess_x_spikes == True:\n","            \n","            if output == 1: # one final check on buying in with wrong signal (minimise potential drawdown)\n","                          \n","                if row['news_spikes_m'] > news_spikes_threshold: # increase confidence that negative drawdown higher       \n","                    output = 0\n","        \n","#         if row[\"percentile_rank_macd_12_26\"] > quantile: # above the historical 75th percentile, strong conviction to buy as momentum serve as a conviction\n","            \n","#             output = 1\n","    \n","    return output\n","\n","# # script 05 general functions\n","\n","# def model_positions(row, sentiment=False, ess_x_spikes=False, quantile=0.7, ess_threshold=0.2, returns_threshold=0.5, news_spikes_threshold=2.0): # checked\n","#     \"\"\"\n","#     Returns position as 1, 0 for Buy and Hold respectively \n","#     based on the predicted price value and threshold\n","#     FYI: predicting forward returns\n","#     \"\"\"\n","\n","#     if sentiment: # using current ess to make positioning for future returns outcome\n","        \n","#         if row['event_sentiment_score'] > ess_threshold: # based on optimal threshold calculated based on 05D monthly datasets\n","#             output = 1\n","#         else:\n","#             output = 0\n","    \n","#     else: # following the score based on predicted returns\n","    \n","#         if row['returns_predict_proba'] > returns_threshold: # same as using returns_predict > 0\n","#             output = 1\n","\n","#         else:\n","#             if row['percentile_rank_ratings_event_sentiment_score'] > quantile or row['percentile_rank_product_event_sentiment_score'] > quantile: # rule based regime override\n","#                 output = 1\n","#             else:\n","#                 output = 0\n","    \n","#         # ultimate check on buy strategy:\n","#         if row['percentile_rank_mergers_event_sentiment_score'] > quantile: # most positive sentiment score \n","\n","#             output = 0 # due to the strong negative correlation for financials\n","        \n","#         if ess_x_spikes == True:\n","            \n","#             if output == 1: # one final check on buying in with wrong signal (minimise potential drawdown)\n","                          \n","#                 if row['news_spikes_m'] > news_spikes_threshold: # increase confidence that negative drawdown higher       \n","#                     output = 0\n","                \n","#     return output\n","\n","def financial_positions(row, ess_x_spikes=False, quantile=0.7, returns_threshold=0.5, news_spikes_threshold=2.0):\n","    \"\"\"\n","    Returns position as 1, 0 for Buy and Hold respectively \n","    based on the combined predicted returns value\n","    \"\"\"\n","\n","    # weighted returns based on AI/ML, using predict_proba\n","    if row['returns_predict'] > returns_threshold: # buy and hold (dont make a move for 1)\n","        output = 1\n","    \n","    else:\n","        output = 0\n","            \n","    return output\n","\n","# def financial_positions(row, ess_x_spikes=False, quantile=0.7, returns_threshold=0.5, news_spikes_threshold=2.0):\n","#     \"\"\"\n","#     Returns position as 1, 0 for Buy and Hold respectively \n","#     based on the combined predicted returns value\n","#     \"\"\"\n","\n","#     # weighted returns based on AI/ML, using predict_proba\n","#     if row['returns_predict'] > returns_threshold: # buy and hold (dont make a move for 1)\n","#         output = 1\n","    \n","#     # rules-based regime on the allocation aggregate with the individual stocks weightng to come out with a final aggregated allocation\n","#     else:\n","#         if row['percentile_rank_ratings_event_sentiment_score'] > quantile or row['percentile_rank_regulatory_event_sentiment_score'] > quantile: # rule based regime override\n","#             output = 1\n","#         else:\n","#             output = 0\n","    \n","#     # ultimate check on buy strategy:\n","#     if row['percentile_rank_product_event_sentiment_score'] > quantile: # most positive sentiment score \n","\n","#         output = 0 # due to the strong negative correlation for financials\n","\n","# #     if ess_x_spikes == True:\n","\n","# #         if output == 1: # one final check on buying in with wrong signal (minimise potential drawdown)\n","\n","# #             if row['news_spikes_m'] > news_spikes_threshold: # increase confidence that negative drawdown higher       \n","# #                 output = 0\n","\n","\n","#     return output\n","\n","# else:\n","#     if row['percentile_rank_regulatory_event_sentiment_score'] > quantile: # rule based regime override\n","#         output = 1\n","#     else:\n","#         output = 0\n","\n","# # ultimate check on buy strategy:\n","# if row['percentile_rank_product_event_sentiment_score'] > quantile: # most positive sentiment score \n","\n","#     output = 0 # due to the strong negative correlation for financials\n","\n","# if ess_x_spikes == True:\n","\n","#     if output == 1: # one final check on buying in with wrong signal (minimise potential drawdown)\n","\n","#         if row['news_spikes_m'] > news_spikes_threshold: # increase confidence that negative drawdown higher       \n","#             output = 0\n","\n","# if row[\"percentile_rank_macd_12_26\"] > quantile: # above the historical 75th percentile, strong conviction to buy as momentum serve as a conviction\n","\n","#     output = 1\n","    \n","\n","# # 1.52 percentage\n","\n","# def financial_positions(row, ess_x_spikes=False, quantile=0.7, returns_threshold=0.5, news_spikes_threshold=2.0):\n","#     \"\"\"\n","#     Returns position as 1, 0 for Buy and Hold respectively \n","#     based on the combined predicted returns value\n","#     \"\"\"\n","\n","#     # weighted returns based on AI/ML, using predict_proba\n","#     if row['returns_predict'] > returns_threshold: # buy and hold (dont make a move for 1)\n","#         output = 1\n","    \n","#     # rules-based regime on the allocation aggregate with the individual stocks weightng to come out with a final aggregated allocation\n","#     else:\n","#         if row['percentile_rank_regulatory_event_sentiment_score'] > quantile: # rule based regime overrideride\n","#             output = 1\n","#         else:\n","#             output = 0\n","    \n","# #     # ultimate check on buy strategy:\n","# #     if row['percentile_rank_product_event_sentiment_score'] > quantile: # most positive sentiment score \n","\n","# #         output = 0 # due to the strong negative correlation for financials\n","\n","# #     if ess_x_spikes == True:\n","\n","# #         if output == 1: # one final check on buying in with wrong signal (minimise potential drawdown)\n","\n","# #             if row['news_spikes_m'] > news_spikes_threshold: # increase confidence that negative drawdown higher       \n","# #                 output = 0\n","\n","\n","#     return output\n","\n","def weight_allocation(df, weight=0.05):\n","    \"\"\"\n","    Assign weights allocation based on threshold, add in additional weights based on model signals\n","    Current: 0.05 +/-\n","    \"\"\"\n","    \n","    df['return_bm'] = df['msci_china_returns']+1\n","    df['return_bm_cum'] = (df['return_bm']).cumprod()\n","    \n","    df['return_mp'] = df.apply(lambda x: x['msci_financial_returns'] * (x['tot_fin_weights'] + weight) \\\n","    + x['msci_other_returns'] * (x['others'] - weight) if x['position_lag_1'] == 1 \\\n","    else x['msci_financial_returns'] * (x['tot_fin_weights'] - weight) \\\n","    + x['msci_other_returns'] * (x['others'] + weight), axis=1)+1\n","\n","#     # The following is for 100% accurate forecasting, ie, knowing the future (just for presentation)\n","#     df['return_mp'] = df.apply(lambda x: x['msci_financial_returns'] * (x['tot_fin_weights'] + weight) \\\n","#     + x['msci_other_returns'] * (x['others'] - weight) if x['msci_financial_returns'] > x['msci_other_returns'] \\\n","#     else x['msci_financial_returns'] * (x['tot_fin_weights'] - weight) \\\n","#     + x['msci_other_returns'] * (x['others'] + weight), axis=1)+1\n","#     df['position'] = df.apply(lambda x: 1 if x['msci_financial_returns'] > x['msci_other_returns'] else 0, axis=1)\n","    \n","    df['return_mp_cum'] = (df['return_mp']).cumprod()\n","    \n","    # excess return mp for winrate calculation\n","    df['excess_return_mp'] = df['return_mp'] - df['return_bm']\n","\n","    # creating strategy directions\n","    df['returns_movement'] = df.apply(lambda x: msci_china_returns_movement(x), axis=1)\n","    df['sedol'] = 'msci_china'                      \n","    \n","    return df\n","\n","def optimize_weight_distribution(df, max_weight=0.15):\n","    \"\"\"\n","    Find optimal figure for weight distribution\n","    Eg, +/- 5%??\n","    \"\"\"\n","    max_weight = int(max_weight*1000) # converting weight to whole number\n","    best_performance = 0.05\n","    best_excess_return = 0.0\n","    \n","    for i in range(5, max_weight+5, 5): # use integer: 0.005 to 0.01\n","        i = i*0.001\n","        print(f'===== Checking weights: {i*100}% =====')\n","        df = weight_allocation(df, i)\n","        performance_list = performance(df, 'msci_china', 'M', True) # set overweight underweight to true\n","        print()\n","\n","        if performance_list[2] > best_excess_return:\n","            \n","            best_performance = i\n","            best_excess_return = performance_list[2] # 3rd element is the annualized excess return (optimize this value)\n","    \n","    return best_performance, best_excess_return\n","\n","def optimize_quantile(df, bank_name, model, sentiment, frequency, quantile=0.95, ess_threshold=0.2, returns_threshold=0.5, news_spikes_threshold=2.0): # checked\n","    \"\"\"\n","    Find optimal quantile for best rules-based signal based on best predicted returns\n","    Customise for individual companies\n","    Percentile optimization starts from 80th percentile onwards\n","    \"\"\"\n","    quantile = int(quantile*100) \n","\n","    best_performance_values = []\n","    best_excess_return = 0.0\n","    \n","    for i in range(80, quantile+5, 5): # use integer: 0.8 to max_quantile\n","        print(f'===== Checking quantile: {i*0.01}% =====')\n","        print()\n","        i = i*0.01\n","        \n","        for j in range(15, 35, 5): # for ess threshold, 0.15 to 0.3\n","            j = j*0.01\n","            \n","            for k in range(40, 65, 5): # for returns threshold, 0.4 to 0.6\n","                k = k*0.01\n","                \n","                for l in range(20, 40, 5): # for news spikes threshold, +2.0 to +3.5\n","                    l = l*0.1\n","\n","                    df =  prediction_processing(df, model, sentiment, frequency, i, j, k, l) # sentiment strategy as True\n","\n","                    performance_list = performance(df, bank_name, 'M')\n","\n","\n","                    if performance_list[2] > best_excess_return:\n","\n","                        best_performance_values = [i, j, k, l]\n","                        best_excess_return = performance_list[2] # 3rd element is the annualized excess return (optimize this value)\n","    \n","    if len(best_performance_values) == 0: # empty list\n","        \n","        best_performance_values = [0.75, 0.2, 0.5, 2.0] # default value for negative excess returns cases\n","        print(\"=== Negative Excess Returns Detected ===\")\n","        \n","    return best_performance_values, best_excess_return\n","\n","# cleaned version of optimize financial quantile (matching with zero rule based regime in the financial model position method)\n","\n","def optimize_financial_quantile(df, bank_name, hk_financials_sedol_list, quantile=0.95): # checked\n","    \"\"\"\n","    Find optimal quantile for best rules-based signal\n","    Customise for individual companies\n","    Percentile optimization starts from 80th percentile onwards\n","    \"\"\"\n","    quantile = int(quantile*100) \n","\n","    best_performance_values = []\n","    best_excess_return = -1\n","    \n","    for i in range(30, 75, 5): # for returns threshold, 0.3 to 0.7)\n","        i = i*0.01\n","        final_df = prediction_processing_financials(df, hk_financials_sedol_list, True, 0.8, i, 2.0)\n","        final_df['sedol'] = bank_name\n","        performance_list = performance(final_df, bank_name, 'M')\n","\n","        if performance_list[2] > best_excess_return:\n","\n","            best_performance_values = [0.8, i, 2.0]\n","            best_excess_return = performance_list[2] # 3rd element is the annualized excess return (optimize this value)\n","\n","    if len(best_performance_values) == 0: # empty list\n","        \n","        best_performance_values = [0.8, 0.6, 2.0] # default value for negative excess returns cases\n","        print(\"=== Negative Excess Returns Greater than -100% Detected ===\")\n","        \n","    return best_performance_values, best_excess_return\n","\n","# def optimize_financial_quantile(df, bank_name, hk_financials_sedol_list, quantile=0.95): # checked\n","#     \"\"\"\n","#     Find optimal quantile for best rules-based signal\n","#     Customise for individual companies\n","#     Percentile optimization starts from 80th percentile onwards\n","#     \"\"\"\n","#     quantile = int(quantile*100) \n","\n","#     best_performance_values = []\n","#     best_excess_return = -1\n","    \n","#     for i in range(80, quantile+5, 5): # use integer: 0.8 to max_quantile\n","#         print(f'===== Checking quantile: {i}% =====')\n","#         print()\n","#         i = i*0.01\n","        \n","#         for j in range(30, 75, 5): # for returns threshold, 0.3 to 0.7\n","#             j = j*0.01\n","            \n","#             for k in range(20, 40, 5): # for news spikes threshold, +2.0 to +3.0\n","#                 k = k*0.1\n","#                 final_df = prediction_processing_financials(df, hk_financials_sedol_list, True, i, j, k)\n","#                 final_df['sedol'] = bank_name\n","#                 performance_list = performance(final_df, bank_name, 'M')\n","\n","#                 if performance_list[2] > best_excess_return:\n","\n","#                     best_performance_values = [i, j, k]\n","#                     best_excess_return = performance_list[2] # 3rd element is the annualized excess return (optimize this value)\n","    \n","#     if len(best_performance_values) == 0: # empty list\n","        \n","#         best_performance_values = [0.75, 0.5, 2.0] # default value for negative excess returns cases\n","#         print(\"=== Negative Excess Returns Greater than -100% Detected ===\")\n","        \n","#     return best_performance_values, best_excess_return\n","\n","# def optimize_financial_quantile(df, bank_name, hk_financials_sedol_list, quantile=0.95): # checked\n","#     \"\"\"\n","#     Find optimal quantile for best rules-based signal\n","#     Customise for individual companies\n","#     Percentile optimization starts from 80th percentile onwards\n","#     \"\"\"\n","#     quantile = int(quantile*100) \n","\n","#     best_performance_values = []\n","#     best_excess_return = -1\n","    \n","#     for i in range(80, quantile+5, 5): # use integer: 0.8 to max_quantile\n","#         print(f'===== Checking quantile: {i}% =====')\n","#         print()\n","#         i = i*0.01\n","        \n","#         for j in range(30, 75, 5): # for returns threshold, 0.3 to 0.7\n","#             j = j*0.01\n","            \n","#             for k in range(20, 40, 5): # for news spikes threshold, +2.0 to +3.0\n","#                 k = k*0.1\n","#                 final_df = prediction_processing_financials(df, hk_financials_sedol_list, True, i, j, k)\n","#                 final_df['sedol'] = bank_name\n","\n","#                 # filtering only relevant columns for weight allocation for msci financials\n","#                 filter_cols = [\n","#                     'position',\n","#                     'position_lag_1',\n","#                     'msci_china_price',\n","#                     'msci_china_returns',\n","#                     'msci_china_returns_lead_1', # for performance measure\n","#                     'msci_financial_price',\n","#                     'msci_financial_returns',\n","#                     'msci_financial_returns_lead_1',\n","#                     \"tot_fin_weights\",\n","#                     \"others\",\n","#                     \"returns_predict\", \n","#                 ]\n","\n","#                 final_df = final_df[filter_cols]\n","#                 final_df['msci_other_returns'] = ((final_df['msci_china_returns'] - final_df['msci_financial_returns']* final_df['tot_fin_weights']/100) / (final_df['others']/100))/100\n","    \n","#                 # Get best model based on performance in excess return from OW/UW strategy\n","#                 final_df = weight_allocation(final_df, 0.1) # using 0.1\n","#                 performance_list = performance(final_df, 'msci_china', 'M', True) # set overweight underweight to true\n","\n","#                 if performance_list[2] >= best_excess_return:\n","\n","#                     best_performance_values = [i, j, k]\n","#                     best_excess_return = performance_list[2] # 3rd element is the annualized excess return (optimize this value)\n","    \n","#     if len(best_performance_values) == 0: # empty list\n","        \n","#         best_performance_values = [0.75, 0.5, 2.0] # default value for negative excess returns cases\n","#         print(\"=== Negative Excess Returns Greater than -100% Detected ===\")\n","        \n","#     return best_performance_values, best_excess_return\n","\n","# def optimize_quantile(df, bank_name, model, sentiment, frequency, max_quantile=0.95):\n","#     \"\"\"\n","#     Find optimal quantile for best rules-based signal\n","#     Customise for individual companies\n","#     Percentile optimization starts from 80th percentile onwards\n","#     \"\"\"\n","#     max_quantile = int(max_quantile*1000) \n","#     best_performance_quantile = 0.75\n","#     best_excess_return = 0.0\n","    \n","#     for i in range(800, max_quantile+5, 5): # use integer: 0.8 to max_quantile\n","        \n","#         i = i*0.001\n","#         print(f'===== Checking quantile: {i*100}% =====')\n","#         df =  prediction_processing(df, model, sentiment, frequency, i) # sentiment strategy as True\n","\n","#         performance_list = performance(df, bank_name, 'M')\n","#         print()\n","\n","#         if performance_list[2] > best_excess_return:\n","            \n","#             best_performance_quantile = i\n","#             best_excess_return = performance_list[2] # 3rd element is the annualized excess return (optimize this value)\n","    \n","#     return best_performance_quantile, best_excess_return\n","\n","### Event Feature Engineering For Event Group\n","- used in 03_Production_Stage_New_Feature_Technical\n","\n","def event_feature_engineering(df): # checked\n","    \"\"\"\n","    Generate event group features for rule-based regime, generalize based on the entire dataset\n","    Note: Percentile Ranking is based off entire duration of the dataframe (not daily, weekly, monthly)\n","    \"\"\"\n","    event_grouping_dict = {\n","        'mergers': ['acquisitions-mergers', 'partnerships'],\n","        'product': ['marketing', 'products-services'],\n","        'earnings': ['dividends', 'earnings', 'revenues', 'credit-ratings'],\n","        'ratings': ['analyst-ratings', 'stock-prices', 'price-targets'],\n","        'regulatory': ['bankruptcy', 'regulatory', 'legal']    \n","    }\n","    \n","    # filter out the important group list\n","    for e, groups in event_grouping_dict.items():\n","        \n","        temp_df = df[df['group'].isin(groups)][['event_sentiment_score', 'model_score']]\n","        temp_df.rename(columns={'event_sentiment_score': f\"{e}_event_sentiment_score\",'model_score': f\"{e}_model_score\" }, inplace=True)\n","        \n","        df = df.merge(temp_df, left_index=True, right_index=True, how='left')\n","        \n","        # Fill NA values as 0\n","        df[f\"{e}_event_sentiment_score\"] = df[f\"{e}_event_sentiment_score\"].fillna(0)\n","        df[f\"{e}_model_score\"] = df[f\"{e}_model_score\"].fillna(0)\n","        \n","    return df\n","\n","def event_percentile_rank(df): # checked\n","    \"\"\"\n","    Create new percentile feature and study the correlation between the returns movement of the price\n","    Note: only use it when aggregated into periodic frequency, eg, based on daily or weekly or monthly\n","    \"\"\"\n","    event_grouping_dict = {\n","        'mergers': ['acquisitions-mergers', 'partnerships'],\n","        'product': ['marketing', 'products-services'],\n","        'earnings': ['dividends', 'earnings', 'revenues', 'credit-ratings'],\n","        'ratings': ['analyst-ratings', 'stock-prices', 'price-targets'],\n","        'regulatory': ['bankruptcy', 'regulatory', 'legal']    \n","    }\n","    \n","    # filter out the important group list\n","    for e, groups in event_grouping_dict.items():\n","        \n","        # percentile rank for the col\n","        df[f\"percentile_rank_{e}_event_sentiment_score\"] = df[f'{e}_event_sentiment_score'].rank(pct=True)\n","        df[f\"percentile_rank_{e}_model_score\"] = df[f'{e}_model_score'].rank(pct=True)\n","     \n","    return df\n","\n","def macd_feature_engineering(df, s):\n","    \"\"\"\n","    To create macd percentile rank per sedol\n","    \"\"\"\n","    # do an inner merge with original dataframe based on index and sedol\n","    df = df[df['sedol'] == i]\n","    df['percentile_rank_macd_12_26'] = df['macd_12_26'].rank(pct=True)\n","    \n","    return df"]}]}